{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import catboost as ctb \n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parquet = \"/home/lishi/projects/Competition/kaggle_2023/data/train_add_daily_features.parquet\"\n",
    "\n",
    "prices =  [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "ta_indicators = ['ema', 'rsi', 'cci', 'mfi', 'ad_osc', 'macd', 'macdhist', 'macdsignal']\n",
    "\n",
    "stock_labels = [0, 1, 2]\n",
    "\n",
    "df_all = pd.read_parquet(train_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11,  31,  70,  80,  82,  85,  86,  92, 100, 101, 129, 158, 174,\n",
       "       177, 180,  78, 153], dtype=int16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shenxian = df_all[df_all['stock_label']==1]['stock_id'].unique()\n",
    "shenxian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.015, #0.009,#0.018,\n",
    "    'max_depth': 12,#9,\n",
    "    'n_estimators': 800,#600,\n",
    "    'num_leaves': 1024,#440,\n",
    "    'objective': 'mae',\n",
    "    'random_state': 42,\n",
    "    'reg_alpha': 0.01,\n",
    "    'reg_lambda': 0.01,\n",
    "    'early_stopping_rounds': None,\n",
    "    'num_threads': 16,\n",
    "    'importance_type': 'gain',\n",
    "    'verbose': -1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2467359/3935730066.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['wap_shift'] = df.groupby(['date_id', 'stock_id'])['wap'].shift(-6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Valid Dates 1: 0 - 47\n",
      "Valid Dates 2: 48 - 96\n",
      "Start training LightGBM...\n",
      "[100]\tvalid_0's l1: 0.122223\n",
      "[200]\tvalid_0's l1: 0.111573\n",
      "[300]\tvalid_0's l1: 0.111038\n",
      "[400]\tvalid_0's l1: 0.111108\n",
      "[500]\tvalid_0's l1: 0.111241\n",
      "[600]\tvalid_0's l1: 0.111509\n",
      "[700]\tvalid_0's l1: 0.111512\n",
      "[800]\tvalid_0's l1: 0.111517\n",
      "Valid MAE: 0.12586312673791855\n",
      "Fold 2\n",
      "Valid Dates 1: 97 - 144\n",
      "Valid Dates 2: 145 - 192\n",
      "Start training LightGBM...\n",
      "[100]\tvalid_0's l1: 0.185495\n",
      "[200]\tvalid_0's l1: 0.165648\n",
      "[300]\tvalid_0's l1: 0.16492\n",
      "[400]\tvalid_0's l1: 0.164833\n",
      "[500]\tvalid_0's l1: 0.164898\n",
      "[600]\tvalid_0's l1: 0.164916\n",
      "[700]\tvalid_0's l1: 0.165009\n",
      "[800]\tvalid_0's l1: 0.165039\n",
      "Valid MAE: 0.15826408476746937\n",
      "Fold 3\n",
      "Valid Dates 1: 193 - 240\n",
      "Valid Dates 2: 241 - 288\n",
      "Start training LightGBM...\n"
     ]
    }
   ],
   "source": [
    "choose_stock = 153 \n",
    "n_split = 5\n",
    "\n",
    "# df = df_all[df_all.stock_id == choose_stock]\n",
    "\n",
    "df = df_all[~df_all['target'].isnull()].reset_index(drop=True)\n",
    "df = df.drop(columns=['row_id', 'time_id'])\n",
    "df['imbalance_buy_sell_flag'] = df['imbalance_buy_sell_flag'].replace({-1: 0, 1: 1})\n",
    "\n",
    "feature_cols = [x for x in df.columns if x not in ['target', 'date_id', 'row_id'] + ta_indicators]\n",
    "category_cols = [\"stock_id\", \"seconds_in_bucket\", 'imbalance_buy_sell_flag', 'stock_label']\n",
    "\n",
    "scale_cols = [x for x in feature_cols if x not in category_cols+['wap']]\n",
    "scaler = StandardScaler().fit(df[scale_cols])\n",
    "df[scale_cols] = scaler.transform(df[scale_cols])\n",
    "\n",
    "scaler_wap = StandardScaler().fit(df[['wap']])\n",
    "df[['wap']] = scaler_wap.transform(df[['wap']])\n",
    "\n",
    "# scaler_target = StandardScaler().fit(df[['target']])\n",
    "# df[['target']] = scaler_target.transform(df[['target']])\n",
    "\n",
    "df['wap_shift'] = df.groupby(['date_id', 'stock_id'])['wap'].shift(-6)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "target_col ='wap_shift'\n",
    "\n",
    "dates_list = df['date_id'].unique()\n",
    "\n",
    "k_fold = KFold(n_splits=n_split, shuffle=False, random_state=None)\n",
    "kf_split = k_fold.split(dates_list)\n",
    "\n",
    "mae_list = []\n",
    "lgb_models = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf_split):\n",
    "    \n",
    "    print(f\"Fold {fold+1}\")\n",
    "\n",
    "    train_dates = dates_list[train_idx]\n",
    "    \n",
    "    half_valid = int(len(valid_idx)/2)\n",
    "    valid_dates_1 = dates_list[valid_idx[:half_valid]]\n",
    "    valid_dates_2 = dates_list[valid_idx[half_valid:]]\n",
    "    \n",
    "    print(f\"Valid Dates 1: {valid_dates_1[0]} - {valid_dates_1[-1]}\")\n",
    "    print(f\"Valid Dates 2: {valid_dates_2[0]} - {valid_dates_2[-1]}\")\n",
    "    \n",
    "    # split train and valid set\n",
    "    df_train_fold = df[df[\"date_id\"].isin(train_dates)].reset_index(drop=True)\n",
    "    df_valid_fold_1 = df[df[\"date_id\"].isin(valid_dates_1)].reset_index(drop=True)\n",
    "    df_valid_fold_2 = df[df[\"date_id\"].isin(valid_dates_2)].reset_index(drop=True)\n",
    "\n",
    "    print(\"Start training LightGBM...\")\n",
    "    lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_model.fit(\n",
    "        df_train_fold[feature_cols].values, \n",
    "        df_train_fold[target_col].values,\n",
    "        eval_set=[(\n",
    "            df_valid_fold_1[feature_cols].values, \n",
    "            df_valid_fold_1[target_col].values\n",
    "            )],\n",
    "        feature_name = feature_cols,\n",
    "        categorical_feature = category_cols,\n",
    "        callbacks=[lgb.callback.log_evaluation(period=100)],\n",
    "        )\n",
    "    \n",
    "    valid_pred_lgb = lgb_model.predict(df_valid_fold_2[feature_cols].values)\n",
    "\n",
    "    # t0_target = df[df['date_id']==valid_dates_1[-1]]['target'].mean()\n",
    "\n",
    "    # targets = \n",
    "\n",
    "    # target_reverse = scaler_target.inverse_transform(df_valid_fold_2[['target']])\n",
    "    # pred_reverse = scaler_target.inverse_transform(valid_pred_lgb.reshape(-1, 1))\n",
    "\n",
    "    valid_mae_lgb = mean_absolute_error(df_valid_fold_2[target_col].values, valid_pred_lgb)\n",
    "    mae_list.append(valid_mae_lgb)\n",
    "    print(f\"Valid MAE: {valid_mae_lgb}\")\n",
    "\n",
    "    lgb_models.append(lgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_stock = 153 \n",
    "select_date = df[\"date_id\"].unique()[18]\n",
    "\n",
    "df_test = df_all[(df_all[\"date_id\"] == select_date) & (df_all.stock_id == choose_stock)].copy().reset_index(drop=True)\n",
    "df_test = df_test.drop(columns=['date_id'])\n",
    "\n",
    "test_pred_lgb = np.mean([\n",
    "    lgb_model.predict(df_test[feature_cols].values) for lgb_model in lgb_models\n",
    "    ], axis=0)\n",
    "\n",
    "# test_pred_lgb = scaler_target.inverse_transform(test_pred_lgb.reshape(-1, 1))\n",
    "\n",
    "df_test['wap_pred'] = test_pred_lgb\n",
    "df_test['wap_pred_reverse'] = scaler_wap.inverse_transform(df_test[['wap_pred']])\n",
    "df_test['wap_reverse'] = scaler_wap.inverse_transform(df_test[['wap']])\n",
    "df_test['wap_shift_reverse'] = scaler_wap.inverse_transform(df_test[['wap_shift']])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(df_test['wap_reverse'], linestyle='dashed', label='wap')\n",
    "ax.plot(df_test['wap_shift_reverse'], label='wap_shift')\n",
    "ax.plot(df_test['wap_pred_reverse'], label='wap_pred')\n",
    "ax.text(0.95, .05, f\"MAE: {mean_absolute_error(df_test['wap_reverse'], df_test['wap_pred_reverse']):.4f}\", transform=ax.transAxes, ha='right', color='green', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance = pd.DataFrame()\n",
    "df_importance['feature'] = lgb_models[0].feature_name_\n",
    "for i, lgb_model in enumerate(lgb_models):\n",
    "    df_importance[f'importance_{i}'] = lgb_model.feature_importances_\n",
    "\n",
    "df_importance['importance'] = df_importance[[f'importance_{i}' for i in range(len(lgb_models))]].mean(axis=1)\n",
    "\n",
    "df_importance = df_importance.sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_importance.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "sns.barplot(data=df_importance.head(40), x='importance', y='feature')\n",
    "plt.axvline(np.percentile(df_importance['importance'], 80), color='red', linestyle='dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thred = np.percentile(df_importance['importance'], 80)\n",
    "np.argmax(df_importance['importance']<thred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_importants = df_importance[df_importance['importance']>=thred]['feature'].values\n",
    "print(len(very_importants))\n",
    "very_importants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ x for x in very_importants if x not in raw_features+v3_features+daily_features ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features = [x for x in very_importants if x in prices+sizes+category_cols ]\n",
    "raw_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_features = [x for x in very_importants if x.endswith('_5')] \n",
    "v3_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_features = [x for x in very_importants if x.endswith('_5d')]\n",
    "daily_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_features), len(v3_features), len(daily_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
