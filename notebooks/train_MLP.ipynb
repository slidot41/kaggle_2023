{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "import os, sys, warnings\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt, ticker\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self,  fc1_num, fc2_num, fc3_num, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1_num = fc1_num\n",
    "        self.fc2_num = fc2_num\n",
    "        self.fc3_num = fc3_num\n",
    "\n",
    "        # 两个全连接层\n",
    "        self.fc1 = nn.Linear(fc1_num, fc2_num) # 702 -> 30\n",
    "        self.fc2 = nn.Linear(fc2_num, 1)\n",
    "        # self.fc3 = nn.Linear(fc3_num, 1)# 30 -> 1\n",
    "        # 激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 初始化权重\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # 使用 xavier 的均匀分布对 weights 进行初始化\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        # 使用正态分布对 bias 进行初始化\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = self.flatten(data)\n",
    "        data = self.fc1(data) # N*30\n",
    "        data = self.relu(data)\n",
    "        data = self.dropout(data)\n",
    "        data = self.fc2(data)\n",
    "        # data = self.relu(data)\n",
    "        # data = self.dropout(data)\n",
    "        # data = self.fc3(data)\n",
    "        # 线性激活函数，无需再进行激活\n",
    "        data = data.to(torch.float)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class FactorData():\n",
    "\n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.len = len(train_x)\n",
    "        self.x_data = train_x\n",
    "        self.y_data = train_y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        指定读取数据的方式：根据索引 index 返回 dataset[index]\n",
    "\n",
    "        \"\"\"\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(model, train_x, train_y, test_x, test_y, lr=1e-4, epoch=30, batch_size=1000, mom=0.9):\n",
    "    # 1. 加载数据\n",
    "    train_dataset = FactorData(train_x, train_y)\n",
    "    test_dataset = FactorData(test_x, test_y)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 2. 模型训练\n",
    "    weight_list, bias_list = [], []\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'bias' in name: \n",
    "            bias_list += [p] # 将所有的 bias 参数放入 bias_list 中\n",
    "        else:\n",
    "            weight_list += [p] # 将所有的 weight 参数放入 weight_list 中\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.MSELoss()\n",
    "    test_criterion = nn.L1Loss()\n",
    "\n",
    "    optimizer = optim.RMSprop(\n",
    "        [{'params': weight_list, 'weight_decay': 1e-5},\n",
    "         {'params': bias_list, 'weight_decay': 0}],\n",
    "        lr=lr,\n",
    "        momentum=mom)\n",
    "    \n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    best_test_epoch, best_test_loss = 0, np.inf\n",
    "    seed = 0\n",
    "\n",
    "    # 开始训练\n",
    "    for epoch in range(1, epoch+1):\n",
    "        # 测试模式\n",
    "        test_loss = 0\n",
    "        model.eval()\n",
    "        test_batch_num = 0\n",
    "        with torch.no_grad():\n",
    "            for data, label in tqdm(test_loader, f'Epoch {epoch}-test ', leave=False):\n",
    "                test_batch_num += 1\n",
    "                data, label = data.to(torch.float), label.to(torch.float)\n",
    "                # 得到测试集的预测值\n",
    "                y_pred = model(data)\n",
    "                # 计算损失\n",
    "                loss = test_criterion(y_pred, label)\n",
    "                # 将损失值加入到本轮测试的损失中\n",
    "                test_loss += loss.item()\n",
    "    \n",
    "    # 在训练集中训练模型\n",
    "    # .train() 参考 https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n",
    "    # .zero_grad() 参考 https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "\n",
    "    train_loss = 0\n",
    "    model.train()  \n",
    "    train_batch_num = 0\n",
    "    for data, label in tqdm(train_loader, f'Epoch {epoch}-train', leave=False):\n",
    "        train_batch_num += 1\n",
    "        # 准备数据\n",
    "        data, label = data.to(torch.float), label.to(torch.float)\n",
    "        # 得到训练集的预测值\n",
    "        out_put = model(data)\n",
    "        # 计算损失\n",
    "        loss = criterion(out_put, label)\n",
    "        # 将损失值加入到本轮训练的损失中\n",
    "        train_loss += loss.item()\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad() \n",
    "        # 反向传播求解梯度\n",
    "        loss.backward()\n",
    "        # 更新权重参数\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss_list.append(train_loss/train_batch_num)\n",
    "    test_loss_list.append(test_loss/test_batch_num)\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据载入到 DataLoader 中\n",
    "batch_size = 1000\n",
    "train_data = FactorData(train_x.values, train_y.values)\n",
    "# train_data = FactorData(train_x_converted, train_y_converted)\n",
    "train_loader = DataLoader(dataset=train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False)  # 不打乱数据集\n",
    "test_data = FactorData(test_x.values, test_y.values)\n",
    "# test_data = FactorData(test_x_converted, test_y_converted)\n",
    "test_loader = DataLoader(dataset=test_data,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)  # 不打乱数据集\n",
    "\n",
    "# 构建模型\n",
    "# alphanet = AlphaNet(combination=combination, combination_rev=combination_rev,\n",
    "#                     index_list=index_list, fc1_num=8820, fc2_num=55, fc3_num=0, dropout_rate=0.2)\n",
    "mlp = MLP(fc1_num=120, fc2_num=60, fc3_num=0, dropout_rate=0.2)\n",
    "weight_list, bias_list = [], []\n",
    "for name, p in mlp.named_parameters():\n",
    "    # 将所有的 bias 参数放入 bias_list 中\n",
    "    if 'bias' in name:\n",
    "        bias_list += [p]\n",
    "    # 将所有的 weight 参数放入 weight_list 中\n",
    "    else:\n",
    "        weight_list += [p]\n",
    "\n",
    "# weight decay: 对所有 weight 参数进行 L2 正则化\n",
    "optimizer = optim.RMSprop([{'params': weight_list, 'weight_decay': 1e-5},\n",
    "                           {'params': bias_list, 'weight_decay': 0}],\n",
    "                          lr=1e-4,\n",
    "                          momentum=0.9)\n",
    "\n",
    "# 损失函数为均方误差 MSE\n",
    "criterion = nn.MSELoss()\n",
    "test_criterion = nn.L1Loss()\n",
    "epoch_num = 50\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "best_test_epoch, best_test_loss = 0, np.inf\n",
    "seed = 0\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    # 测试模式\n",
    "    test_loss = 0\n",
    "    mlp.eval()\n",
    "    test_batch_num = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in tqdm(test_loader, f'Epoch {epoch}-test ', leave=False):\n",
    "            test_batch_num += 1\n",
    "            data, label = data.to(torch.float), label.to(torch.float)\n",
    "            # 得到测试集的预测值\n",
    "            y_pred = mlp(data)\n",
    "            # 计算损失\n",
    "            loss = test_criterion(y_pred, label)\n",
    "            # 将损失值加入到本轮测试的损失中\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    train_loss = 0\n",
    "    # 在训练集中训练模型\n",
    "    mlp.train()  # 关于。train() 的作用，可以参考 https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n",
    "    train_batch_num = 0\n",
    "    for data, label in tqdm(train_loader, f'Epoch {epoch}-train', leave=False):\n",
    "        train_batch_num += 1\n",
    "        # 准备数据\n",
    "        data, label = data.to(torch.float), label.to(torch.float)\n",
    "        # 得到训练集的预测值\n",
    "        out_put = mlp(data)\n",
    "        # 计算损失\n",
    "        loss = criterion(out_put, label)\n",
    "        # 将损失值加入到本轮训练的损失中\n",
    "        train_loss += loss.item()\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad() # 关于。zero_grad() 的作用，可以参考 https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        # 反向传播求解梯度\n",
    "        loss.backward()\n",
    "        # 更新权重参数\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_loss_list.append(train_loss/train_batch_num)\n",
    "    test_loss_list.append(test_loss/test_batch_num)\n",
    "print(train_loss_list)\n",
    "print(test_loss_list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
