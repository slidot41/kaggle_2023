{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockApi:\n",
    "    def __init__(self, input_paths: Sequence[str], gid_col: str, export_gid_col: bool):\n",
    "        '''\n",
    "        YOU MUST UPDATE THE FIRST THREE LINES of this method.\n",
    "        They've been intentionally left in an invalid state.\n",
    "\n",
    "        Variables to set:\n",
    "            input_paths: a list of two or more paths to the csv files to be served\n",
    "            group_id_column: the column that identifies which groups of rows the API should serve.\n",
    "                A call to iter_test serves all rows of all dataframes with the current group ID value.\n",
    "            export_group_id_column: if true, the dataframes iter_test serves will include the group_id_column values.\n",
    "        '''\n",
    "        self.input_paths: Sequence[str] = input_paths\n",
    "        self.group_id_column: str = gid_col\n",
    "        self.export_group_id_column: bool = export_gid_col\n",
    "        # iter_test is only designed to support at least two dataframes, such as test and sample_submission\n",
    "        assert len(self.input_paths) >= 2\n",
    "\n",
    "        self._status = 'initialized'\n",
    "        self.predictions = []\n",
    "\n",
    "    def iter_test(self) -> Tuple[pd.DataFrame]:\n",
    "        '''\n",
    "        Loads all of the dataframes specified in self.input_paths,\n",
    "        then yields all rows in those dataframes that equal the current self.group_id_column value.\n",
    "        '''\n",
    "        if self._status != 'initialized':\n",
    "            raise Exception('WARNING: the real API can only iterate over `iter_test()` once.')\n",
    "\n",
    "        dataframes = []\n",
    "        for pth in self.input_paths:\n",
    "            dataframes.append(pd.read_csv(pth, low_memory=False))\n",
    "        group_order = dataframes[0][self.group_id_column].drop_duplicates().tolist()\n",
    "        dataframes = [df.set_index(self.group_id_column) for df in dataframes]\n",
    "\n",
    "        for group_id in group_order:\n",
    "            self._status = 'prediction_needed'\n",
    "            current_data = []\n",
    "            for df in dataframes:\n",
    "                cur_df = df.loc[group_id].copy()\n",
    "                # returning single line dataframes from df.loc requires special handling\n",
    "                if not isinstance(cur_df, pd.DataFrame):\n",
    "                    cur_df = pd.DataFrame({a: b for a, b in zip(cur_df.index.values, cur_df.values)}, index=[group_id])\n",
    "                    cur_df.index.name = self.group_id_column\n",
    "                cur_df = cur_df.reset_index(drop=not(self.export_group_id_column))\n",
    "                current_data.append(cur_df)\n",
    "            yield tuple(current_data)\n",
    "\n",
    "            while self._status != 'prediction_received':\n",
    "                print('You must call `predict()` successfully before you can continue with `iter_test()`', flush=True)\n",
    "                yield None\n",
    "\n",
    "        with open('submission.csv', 'w') as f_open:\n",
    "            pd.concat(self.predictions).to_csv(f_open, index=False)\n",
    "        self._status = 'finished'\n",
    "\n",
    "    def predict(self, user_predictions: pd.DataFrame):\n",
    "        '''\n",
    "        Accepts and stores the user's predictions and unlocks iter_test once that is done\n",
    "        '''\n",
    "        if self._status == 'finished':\n",
    "            raise Exception('You have already made predictions for the full test set.')\n",
    "        if self._status != 'prediction_needed':\n",
    "            raise Exception('You must get the next test sample from `iter_test()` first.')\n",
    "        if not isinstance(user_predictions, pd.DataFrame):\n",
    "            raise Exception('You must provide a DataFrame.')\n",
    "\n",
    "        self.predictions.append(user_predictions)\n",
    "        self._status = 'prediction_received'\n",
    "        \n",
    "    def reset_status(self):\n",
    "        self._status = 'initialized'\n",
    "\n",
    "\n",
    "def make_env(input_paths: Sequence[str], gid_col: str, export_gid_col: bool):\n",
    "    return MockApi(input_paths, gid_col, export_gid_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_features_online(test_df, cache, feature_dicts, max_ts_len, ):\n",
    "    \n",
    "    prices = feature_dicts['prices']\n",
    "    sizes = feature_dicts['sizes']\n",
    "    categories = feature_dicts['categories']\n",
    "    \n",
    "    df_v1, v1_features, v1_feature_category = gen_v1_features(test_df, prices)\n",
    "    feature_dicts['v1_features'] = v1_features\n",
    "    feature_dicts['v1_feature_category'] = v1_feature_category\n",
    "    \n",
    "    df_v2, v2_features = gen_v2_features(test_df, sizes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = ['test.csv', 'revealed_targets.csv', 'sample_submission.csv']\n",
    "input_paths = [f'/home/lishi/projects/Competition/kaggle_2023/data/example_test_files/{x}' for x in input_files]\n",
    "\n",
    "env = make_env(input_paths, gid_col='time_id', export_gid_col=True)\n",
    "\n",
    "for (test_df, revealed_targets, sample_prediction_df) in env.iter_test():\n",
    "    print(test_df)\n",
    "    print(sample_prediction_df)\n",
    "    env.predict(sample_prediction_df)\n",
    "    env.reset_status()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import talib as ta\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "import os, sys, warnings\n",
    "from time import time \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "import sys \n",
    "# add path of optiver2023 package to pythonpath \n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "\n",
    "import optiver2023 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    # Calculate the initial memory usage of the DataFrame\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                # Check if the column's data type is a float\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                    \n",
    "    if verbose:\n",
    "        print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_v1_features(df, prices):\n",
    "\n",
    "    # V1 features: directly apply formula to a single row\n",
    "    v1_features = {\n",
    "        \"volume\": \"ask_size + bid_size\",\n",
    "        \"mid_price\": \"(ask_price + bid_price)/2\",\n",
    "        \"liquidity_imbalance\": \"(bid_size-ask_size)/(bid_size+ask_size)\",\n",
    "        \"matched_imbalance\": \"(imbalance_size - matched_size)/(matched_size+imbalance_size)\",\n",
    "        \"size_imbalance\": \"bid_size / ask_size\",\n",
    "        \"imbalance_intensity\": \"imbalance_size / volume\",\n",
    "        \"matched_intensity\": \"matched_size / volume\",\n",
    "        \"price_spread\": \"ask_price - bid_price\",\n",
    "        'market_urgency': 'price_spread * liquidity_imbalance',\n",
    "        'depth_pressure': '(ask_size - bid_size) * (far_price - near_price)',\n",
    "        'price_pressure': 'imbalance_size * (ask_price - bid_price)',\n",
    "        'imbalance_with_flag': 'imbalance_size * imbalance_buy_sell_flag',\n",
    "    }\n",
    "\n",
    "    # include pair-wise price imbalances\n",
    "    for c in combinations(prices, 2):\n",
    "        v1_features[f\"{c[0]}_{c[1]}_imbalance\"] = f\"({c[0]} - {c[1]}) / ({c[0]} + {c[1]})\"\n",
    "    \n",
    "    for k, v in v1_features.items():\n",
    "        df[k] = df.eval(v)\n",
    "        \n",
    "    v1_feature_category = {\n",
    "        'minute': 'seconds_in_bucket // 60',\n",
    "        'imb_buy_side': \"(imbalance_buy_sell_flag == 1)\",\n",
    "        'imb_sell_side': \"(imbalance_buy_sell_flag == -1)\",\n",
    "        'first_half_session': '(seconds_in_bucket <= 240)',\n",
    "        'second_half_session': '(seconds_in_bucket > 240)'\n",
    "    }\n",
    "    \n",
    "    for k, v in v1_feature_category.items():\n",
    "        df[k] = df.eval(v).astype(np.int8)\n",
    "        \n",
    "    df = reduce_mem_usage(df, verbose=0)\n",
    "        \n",
    "    return df, list(v1_features.keys()), list(v1_feature_category.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_v2_features(df, v2_feat_cols):\n",
    "    \n",
    "    # V2 features: cross-section features\n",
    "    # V2 features are generated on the groupby(['date_id', 'seconds_in_bucket'])\n",
    "    # These features includes:\n",
    "    # 1. statistics of V1 features (non-categorical)\n",
    "    # 2. rank of V1 features for each stocks (non-categorical)\n",
    "    group = df.groupby(['date_id', 'seconds_in_bucket'])\n",
    "\n",
    "    v2_features_stats = ['mean', 'median', 'std', 'min', 'max']\n",
    "\n",
    "    # calculate statistics of V1 features for each stock\n",
    "    df_v2 = group[v2_feat_cols].agg(v2_features_stats).reset_index()\n",
    "    df_v2.columns = ['date_id', 'seconds_in_bucket'] + [f\"{c[1]}_{c[0]}\" for c in df_v2.columns[2:]]\n",
    "    df = df.merge(df_v2, on=['date_id', 'seconds_in_bucket'], how='left')\n",
    "    \n",
    "    # calculate rank of V1 features for each stock\n",
    "    df_v2 = group[v2_feat_cols].rank(pct=True).add_prefix('rank_')\n",
    "    df = df.merge(df_v2, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    df = reduce_mem_usage(df, verbose=0)\n",
    "    \n",
    "    v2_features =\\\n",
    "        [f\"{s}_{c}\" for c in v2_feat_cols for s in v2_features_stats] + \\\n",
    "        [f\"rank_{c}\" for c in v2_feat_cols]\n",
    "        \n",
    "    return df, v2_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Requrires at least 11 timesteps to calculate all rolling statistics\n",
    "def gen_v3_features(df, prices, sizes, v1_features):\n",
    "    # V3 features: rolling statistics of V1 features (non-categorical)\n",
    "    # V3 features are generated on the groupby(['date_id', 'stock_id'])\n",
    "    # here we introduce ta-lib functions to calculate TA indicators\n",
    "\n",
    "    # V3.1 relative change of V1 features by shift(1)\n",
    "    # for prices, we calculate the change in basis points (*1e4)\n",
    "    # for other features, we calculate the change in percentage (*1e2)\n",
    "    group_by_stock = df.groupby(['date_id', 'stock_id'])\n",
    "    \n",
    "    relative_price = group_by_stock[prices].pct_change(1).add_prefix('pct_')*1e4\n",
    "    relative_others = group_by_stock[sizes+v1_features].pct_change(1).add_prefix('pct_')*1e2\n",
    "\n",
    "    df = pd.concat([df, relative_price, relative_others], axis=1)\n",
    "    v3_features = list(relative_price.columns) + list(relative_others.columns)\n",
    "    \n",
    "    # V3.2 Simple TA indicators\n",
    "    # Those are simple TA indicators that use only one feature\n",
    "    df_v3 = group_by_stock[prices + sizes + v1_features].rolling(5).agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "    stats_cols = [f\"{c[1]}_{c[0]}_5\" for c in df_v3.columns[2:]]\n",
    "    df_v3.columns = ['date_id', 'stock_id'] + stats_cols\n",
    "    df_v3.set_index('_level_2_5', inplace=True)\n",
    "    df_v3.drop(columns=['date_id', 'stock_id'], inplace=True)\n",
    "    \n",
    "    df = df.merge(df_v3, left_index=True, right_index=True, how='left')\n",
    "    v3_features += df_v3.columns.tolist()\n",
    "        \n",
    "    # # V3.3 TA indicators that use multiple features\n",
    "    def composite_ta(df):\n",
    "\n",
    "        ad_osc = ta.ADOSC(df['ask_price'], df['bid_price'], df['wap'], df['volume'], fastperiod=3, slowperiod=5)\n",
    "        macd, macdsignal, macdhist = ta.MACD(df['wap'], fastperiod=5, slowperiod=11, signalperiod=3)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'ema': ta.EMA(df['wap'], timeperiod=5),\n",
    "            'rsi': ta.RSI(df['wap'], timeperiod=5),\n",
    "            'cci': ta.CCI(df['ask_price'], df['bid_price'], df['wap'], timeperiod=5),\n",
    "            'mfi': ta.MFI(df['ask_price'], df['bid_price'], df['wap'], df['volume'], timeperiod=5),\n",
    "            'ad_osc': ad_osc,\n",
    "            'macd': macd,\n",
    "            'macdsignal': macdsignal,\n",
    "            'macdhist': macdhist\n",
    "        })\n",
    "    \n",
    "    df_v3 = group_by_stock.apply(composite_ta) \n",
    "    v3_features += df_v3.columns.tolist()\n",
    "    \n",
    "    df_v3.reset_index(inplace=True)\n",
    "    df_v3.set_index('level_2', inplace=True)\n",
    "    df_v3.drop(columns=['date_id', 'stock_id'], inplace=True)\n",
    "    \n",
    "    df = pd.concat([df, df_v3], axis=1)\n",
    "    \n",
    "    return df, v3_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "Targets revealed for day [477]\n",
      "New Day! Time used: 41.040879s.\n",
      "Targets revealed for day [478]\n",
      "(11200, 463)\n",
      "New Day! Time used: 52.635002s.\n",
      "Targets revealed for day [479]\n",
      "(18600, 463)\n",
      "New Day! Time used: 79.226819s.\n"
     ]
    }
   ],
   "source": [
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()\n",
    "counter = 0 \n",
    "max_ts_len = 12 # max length of ts to keep in cache\n",
    "\n",
    "n_reveals = 0\n",
    "\n",
    "feature_dicts = {\n",
    "    'prices': [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"],\n",
    "    'sizes':  [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"],\n",
    "    \"category\": [\"stock_id\", \"seconds_in_bucket\", 'imbalance_buy_sell_flag']\n",
    "}\n",
    "\n",
    "cache = pd.DataFrame()\n",
    "df_records = pd.DataFrame()\n",
    "\n",
    "day_begin = time()\n",
    "\n",
    "for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "    \n",
    "    now_time = time()\n",
    "    \n",
    "    current_sec = test['seconds_in_bucket'].unique()\n",
    "\n",
    "    test = test.fillna(0)\n",
    "    test = reduce_mem_usage(test, verbose=0)\n",
    "    \n",
    "    df_v1, v1_feat, v1_feat_cat = gen_v1_features(test, feature_dicts['prices'])\n",
    "    feature_dicts['v1_features'] = v1_feat\n",
    "    feature_dicts['v1_feature_category'] = v1_feat_cat\n",
    "    \n",
    "    v2_feat_cols = feature_dicts['prices'] + feature_dicts['sizes'] + feature_dicts['v1_features']\n",
    "    df_v2, v2_features = gen_v2_features(df_v1, v2_feat_cols)\n",
    "    feature_dicts['v2_features'] = v2_features\n",
    "    \n",
    "    cache = pd.concat([cache, df_v2])\n",
    "    cache.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # In cache, we keep only the past max_ts_len seconds of data\n",
    "    if counter > max_ts_len:\n",
    "        sec_in_buk_list = cache['seconds_in_bucket'].unique()\n",
    "        sec_in_buk_list.sort()\n",
    "        sec_to_keep = sec_in_buk_list[-max_ts_len:]\n",
    "        cache = cache[cache['seconds_in_bucket'].isin(sec_to_keep)]\n",
    "        cache.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    df_v3, v3_features = gen_v3_features(\n",
    "        cache, \n",
    "        feature_dicts['prices'],\n",
    "        feature_dicts['sizes'],\n",
    "        feature_dicts['v1_features']\n",
    "        )\n",
    "    \n",
    "    feature_dicts['v3_features'] = v3_features\n",
    "    \n",
    "    df_v3.fillna(0, inplace=True)\n",
    "    df_v3.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    df_v3 = reduce_mem_usage(df_v3, verbose=0)\n",
    "    \n",
    "    df_test = df_v3[df_v3['seconds_in_bucket'].isin(current_sec)]\n",
    "    df_records = pd.concat([df_records, df_test])\n",
    "    \n",
    "    sample_prediction['target'] = 0.5\n",
    "    \n",
    "    env.predict(sample_prediction)\n",
    "\n",
    "    # after 54 timesteps, a new day starts\n",
    "    if counter >= 54:\n",
    "        print(f\"New Day! Time used: {time() - day_begin:2f}s.\")\n",
    "        counter = 0\n",
    "        day_begin = time()\n",
    "    else:\n",
    "        counter += 1\n",
    "        \n",
    "    if counter == 1:\n",
    "        n_reveals += 1        \n",
    "        print('Targets revealed for day', revealed_targets['revealed_date_id'].unique().tolist())\n",
    "        if n_reveals > 1:\n",
    "            df_records.merge(\n",
    "                revealed_targets, \n",
    "                left_on=['date_id', 'stock_id', 'seconds_in_bucket'],\n",
    "                right_on=['revealed_date_id', 'stock_id', 'seconds_in_bucket'], \n",
    "                how='left')\n",
    "            print(df_records.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28200, 463)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_records.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\", nrows=10000)\n",
    "df = df[~df['target'].isnull()] \n",
    "\n",
    "df['far_price'] = df['far_price'].fillna(0)\n",
    "df['near_price'] = df['near_price'].fillna(0)\n",
    "\n",
    "df = reduce_mem_usage(df, verbose=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = df.groupby(['date_id', 'seconds_in_bucket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_feat_cols = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "\n",
    "v2_features_stats = ['mean', 'median', 'std', 'min', 'max']\n",
    "\n",
    "# calculate statistics of V1 features for each stock\n",
    "df_v2 = group[v2_feat_cols].agg(v2_features_stats).reset_index()\n",
    "df_v2.columns = ['date_id', 'seconds_in_bucket'] + [f\"{c[1]}_{c[0]}\" for c in df_v2.columns[2:]]\n",
    "# df = df.merge(df_v2, on=['date_id', 'seconds_in_bucket'], how='left')\n",
    "df_v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df[(df['date_id'] == 0) & (df['seconds_in_bucket'] == 0)]\n",
    "\n",
    "sub_df.groupby(['date_id', 'seconds_in_bucket'])[v2_feat_cols].agg(v2_features_stats).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
