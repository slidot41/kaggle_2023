{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据载入到 DataLoader 中\n",
    "batch_size = 1000\n",
    "train_data = FactorData(train_x.values, train_y.values)\n",
    "# train_data = FactorData(train_x_converted, train_y_converted)\n",
    "train_loader = DataLoader(dataset=train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False)  # 不打乱数据集\n",
    "test_data = FactorData(test_x.values, test_y.values)\n",
    "# test_data = FactorData(test_x_converted, test_y_converted)\n",
    "test_loader = DataLoader(dataset=test_data,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)  # 不打乱数据集\n",
    "\n",
    "# 构建模型\n",
    "# alphanet = AlphaNet(combination=combination, combination_rev=combination_rev,\n",
    "#                     index_list=index_list, fc1_num=8820, fc2_num=55, fc3_num=0, dropout_rate=0.2)\n",
    "mlp = MLP(fc1_num=120, fc2_num=60, fc3_num=0, dropout_rate=0.2)\n",
    "weight_list, bias_list = [], []\n",
    "for name, p in mlp.named_parameters():\n",
    "    # 将所有的 bias 参数放入 bias_list 中\n",
    "    if 'bias' in name:\n",
    "        bias_list += [p]\n",
    "    # 将所有的 weight 参数放入 weight_list 中\n",
    "    else:\n",
    "        weight_list += [p]\n",
    "\n",
    "# weight decay: 对所有 weight 参数进行 L2 正则化\n",
    "optimizer = optim.RMSprop([{'params': weight_list, 'weight_decay': 1e-5},\n",
    "                           {'params': bias_list, 'weight_decay': 0}],\n",
    "                          lr=1e-4,\n",
    "                          momentum=0.9)\n",
    "\n",
    "# 损失函数为均方误差 MSE\n",
    "criterion = nn.MSELoss()\n",
    "test_criterion = nn.L1Loss()\n",
    "epoch_num = 50\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "best_test_epoch, best_test_loss = 0, np.inf\n",
    "seed = 0\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    # 测试模式\n",
    "    test_loss = 0\n",
    "    mlp.eval()\n",
    "    test_batch_num = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in tqdm(test_loader, f'Epoch {epoch}-test ', leave=False):\n",
    "            test_batch_num += 1\n",
    "            data, label = data.to(torch.float), label.to(torch.float)\n",
    "            # 得到测试集的预测值\n",
    "            y_pred = mlp(data)\n",
    "            # 计算损失\n",
    "            loss = test_criterion(y_pred, label)\n",
    "            # 将损失值加入到本轮测试的损失中\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    train_loss = 0\n",
    "    # 在训练集中训练模型\n",
    "    mlp.train()  # 关于。train() 的作用，可以参考 https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n",
    "    train_batch_num = 0\n",
    "    for data, label in tqdm(train_loader, f'Epoch {epoch}-train', leave=False):\n",
    "        train_batch_num += 1\n",
    "        # 准备数据\n",
    "        data, label = data.to(torch.float), label.to(torch.float)\n",
    "        # 得到训练集的预测值\n",
    "        out_put = mlp(data)\n",
    "        # 计算损失\n",
    "        loss = criterion(out_put, label)\n",
    "        # 将损失值加入到本轮训练的损失中\n",
    "        train_loss += loss.item()\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad() # 关于。zero_grad() 的作用，可以参考 https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        # 反向传播求解梯度\n",
    "        loss.backward()\n",
    "        # 更新权重参数\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_loss_list.append(train_loss/train_batch_num)\n",
    "    test_loss_list.append(test_loss/test_batch_num)\n",
    "print(train_loss_list)\n",
    "print(test_loss_list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
