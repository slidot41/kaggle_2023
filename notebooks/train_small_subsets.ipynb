{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, sys, warnings\n",
    "from time import time \n",
    "from create_feature import *\n",
    "\n",
    "import lightgbm as lgb\n",
    "import joblib, gc\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(csv_file, feature_dicts, feature_versions=['v1', 'v2', 'v3'], nrows=None, save_csv=None):\n",
    "    \n",
    "    df = pd.read_csv(csv_file, nrows=nrows)\n",
    "    df = df[~df['target'].isnull()] \n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    df = reduce_mem_usage(df, verbose=0)\n",
    "    \n",
    "    print(df.shape)\n",
    "    print(f\"Trading days: {df['date_id'].nunique()}\")\n",
    "    print(f\"Stocks: {df['stock_id'].nunique()}\")\n",
    "    \n",
    "    if 'v1' in feature_versions:\n",
    "        df, v1_feat, v1_feat_cat = gen_v1_features(df, feature_dicts['prices'])\n",
    "        feature_dicts['v1_features'] = v1_feat\n",
    "        feature_dicts['v1_feature_category'] = v1_feat_cat\n",
    "    \n",
    "    if 'v2' in feature_versions:\n",
    "        v2_feat_cols = feature_dicts['prices'] + feature_dicts['sizes'] + feature_dicts['v1_features']\n",
    "        df, v2_features = gen_v2_features(df, v2_feat_cols)\n",
    "        feature_dicts['v2_features'] = v2_features\n",
    "        \n",
    "    if 'v3' in feature_versions:\n",
    "        df, v3_features = gen_v3_features(\n",
    "            df, \n",
    "            feature_dicts['prices'],\n",
    "            feature_dicts['sizes'],\n",
    "            feature_dicts['v1_features']\n",
    "            )\n",
    "        \n",
    "        feature_dicts['v3_features'] = v3_features\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    df = reduce_mem_usage(df, verbose=0)\n",
    "    \n",
    "    return df, feature_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 17)\n",
      "Trading days: 1\n",
      "Stocks: 191\n",
      "354\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "feature_dicts = {\n",
    "    'prices': [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"],\n",
    "    'sizes':  [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"],\n",
    "    \"category\": [\"stock_id\", \"seconds_in_bucket\", 'imbalance_buy_sell_flag']\n",
    "    }\n",
    "\n",
    "feat_version = ['v1', 'v2', 'v3']\n",
    "\n",
    "train_csv = \"/home/lishi/projects/Competition/kaggle_2023/data/train.csv\"\n",
    "\n",
    "df, feature_dicts = prepare_data(train_csv, feature_dicts, feat_version, nrows=1000, save_csv=None)\n",
    "\n",
    "feature_cols, category_cols = gen_feature_cols(feature_dicts)\n",
    "\n",
    "print(len(feature_cols))\n",
    "print(len(category_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ids = df['date_id'].unique()\n",
    "\n",
    "date_range_len = 55\n",
    "date_ranges = [(date_ids[i], date_ids[i+date_range_len]) for i in range(len(date_ids)-date_range_len)]\n",
    "\n",
    "print(len(date_ranges))\n",
    "\n",
    "valid_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_cross_validate(df_train, df_valid, feature_cols, category_cols, lgb_params, model_name, save_dir, scaler_file=None, n_splits=5):\n",
    "    \n",
    "    scale_cols = [x for x in feature_cols if x not in category_cols]\n",
    "    scaler = StandardScaler().fit(df_train[scale_cols])\n",
    "    \n",
    "    if scaler_file:\n",
    "        joblib.dump(scaler, scaler_file)\n",
    "\n",
    "    df_train[scale_cols] = scaler.transform(df_train[scale_cols])\n",
    "    df_valid[scale_cols] = scaler.transform(df_valid[scale_cols])\n",
    "    \n",
    "    check_invalids = pd.DataFrame(columns=['null', 'inf'])\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            check_invalids.loc[col] = [df_train[col].isnull().sum(), np.isinf(df_train[col]).sum()]\n",
    "        except:\n",
    "            print(\"Skip column: \", col)\n",
    "            pass\n",
    "        \n",
    "    has_invalids = check_invalids.T[check_invalids.sum(axis=0)!=0]\n",
    "    \n",
    "    if len(has_invalids) > 0:\n",
    "        print(\"Invalid values were found in dataframe.\")\n",
    "        print(has_invalids)\n",
    "        raise Exception(\"Invalid values in dataframe\")\n",
    "    \n",
    "    dates_list = df_train['date_id'].unique()\n",
    "    k_fold = KFold(n_splits=n_splits, shuffle=False, random_state=None)\n",
    "    kf_split = k_fold.split(dates_list)\n",
    "    \n",
    "    mae_scores = []\n",
    "    models = []\n",
    "    \n",
    "    print(df_train['date_id'].unique())\n",
    "    \n",
    "    print(\"Start Cross-validation...\")\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf_split):\n",
    "        \n",
    "        train_dates = dates_list[train_idx]\n",
    "        valid_dates = dates_list[valid_idx]\n",
    "        \n",
    "        print(f\"Fold {fold+1}\")\n",
    "        fold_start = time()\n",
    "        \n",
    "        # split train and valid set\n",
    "        df_train_fold = df_train[df_train[\"date_id\"].isin(train_dates)]\n",
    "        df_valid_fold = df_train[df_train[\"date_id\"].isin(valid_dates)]\n",
    "        \n",
    "        print(f\"Train : {df_train_fold.shape}, Valid : {df_valid_fold.shape}\")\n",
    "        \n",
    "        print(f\"Data preparation finished. Start training...\")\n",
    "        \n",
    "        training_start = time()\n",
    "        \n",
    "        lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "        \n",
    "        lgb_model.fit(\n",
    "            df_train_fold[feature_cols], \n",
    "            df_train_fold['target'],\n",
    "            eval_set=[(df_valid_fold[feature_cols], df_valid_fold['target'])],\n",
    "            feature_name = feature_cols,\n",
    "            categorical_feature = category_cols,\n",
    "            callbacks=[lgb.callback.log_evaluation(period=100)],\n",
    "            )\n",
    "        \n",
    "        models.append(lgb_model)\n",
    "        \n",
    "        model_file = f\"{save_dir}/{model_name}_fold_{fold+1}.pkl\" \n",
    "        joblib.dump(lgb_model, model_file)\n",
    "        \n",
    "        print(f\"Fold {fold+1} Trainning finished. Time elapsed: {time()-training_start:.2f}s\")\n",
    "        \n",
    "        y_pred_valid = lgb_model.predict(df_valid[feature_cols])\n",
    "        mae = mean_absolute_error(df_valid['target'].values, y_pred_valid)\n",
    "        mae_scores.append(mae)\n",
    "\n",
    "        print(f\"Fold {fold+1} MAE: {mae}\")\n",
    "        print(f\"Fold {fold+1} Time elapsed: {time()-fold_start:.2f}s\")\n",
    "        \n",
    "        del df_train_fold, df_valid_fold, y_pred_valid\n",
    "        gc.collect()\n",
    "        \n",
    "    return models, mae_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_subdf(df, feature_cols, category_cols, date_range, valid_len, lgb_params):\n",
    "    \n",
    "    print(date_range)\n",
    "\n",
    "    df_train = df[(df['date_id'] >= date_range[0]) & (df['date_id'] < (date_range[1]-valid_len))]\n",
    "    df_valid = df[(df['date_id'] >= (date_range[1]-valid_len)) & (df['date_id'] < date_range[1])]\n",
    "    \n",
    "    print(f\"Train days: {df_train['date_id'].nunique()}\")\n",
    "    print(f\"Valid days: {df_valid['date_id'].nunique()}\")\n",
    "    \n",
    "    cv_results = train_and_cross_validate(\n",
    "        df_train,\n",
    "        df_valid,\n",
    "        feature_cols, \n",
    "        category_cols, \n",
    "        lgb_params, \n",
    "        model_name = f\"small_lgb_{date_range[0]}_{date_range[1]}_{valid_len}\",\n",
    "        save_dir = \"../data/small_lgbm/\", \n",
    "        scaler_file=\"../data/small_lgb_cv_scaler.pkl\", \n",
    "        n_splits=5)\n",
    "    \n",
    "    return cv_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set lgb parameters\n",
    "lgb_params = {\n",
    "    'learning_rate': 0.01,#0.018,\n",
    "    'max_depth': 7,#9,\n",
    "    'n_estimators': 500,#600,\n",
    "    'num_leaves': 70,#440,\n",
    "    'objective': 'mae',\n",
    "    'random_state': 42,\n",
    "    'reg_alpha': 0.01,\n",
    "    'reg_lambda': 0.01,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'num_threads': 6,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'importance_type': 'gain',\n",
    "    'verbose': -1,\n",
    "    }\n",
    "\n",
    "csv_results = []\n",
    "\n",
    "for d_range in date_ranges[11::3]:\n",
    "    csv_result = train_on_subdf(df, feature_cols, category_cols, d_range, valid_len, lgb_params)\n",
    "    print(f\"\\n{'-'*10}\\nDate range {d_range} finished.\")\n",
    "    print(f\"Mean MAE: {np.mean(csv_result[1])}\\n{'-'*10}\\n\")\n",
    "    csv_results.append(csv_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_list = [np.mean(x[1]) for x in csv_results]\n",
    "\n",
    "print(f\"Mean MAE: {np.mean(mae_list)}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(mae_list)\n",
    "ax.set_xticks(np.arange(0, len(mae_list), 3))\n",
    "xtick_labels = [f\"{x[0]}-{x[1]}\" for x in date_ranges[11::3]]\n",
    "ax.set_xticklabels(xtick_labels[0:len(mae_list):3], rotation=45)\n",
    "ax.set_xlabel(\"Date range\")\n",
    "ax.set_ylabel(\"MAE\")\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove incomplete dates \n",
    "df = df[df['date_id'] != df['date_id'].max()]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "valid_lenght = 5 # days\n",
    "\n",
    "df_train = df[df['date_id'] < df['date_id'].max() - valid_lenght]\n",
    "df_valid = df[df['date_id'] >= df['date_id'].max() - valid_lenght]\n",
    "\n",
    "print(df_train.shape, df_train['date_id'].unique())\n",
    "print(df_valid.shape, df_valid['date_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cv_results = train_and_cross_validate(\n",
    "#     df_train,\n",
    "#     df_valid,\n",
    "#     feature_cols, \n",
    "#     category_cols, \n",
    "#     lgb_params, \n",
    "#     model_name = \"small_lgb_cv\", \n",
    "#     save_dir = \"../data\", \n",
    "#     scaler_file=\"../data/small_lgb_cv_scaler.pkl\", \n",
    "#     n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mae scores\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "mae_scores = cv_results[1]\n",
    "\n",
    "plt.plot(mae_scores, marker='o', color='blue', label='MAE')\n",
    "plt.title(f'MAE Scores(Overall: {np.mean(mae_scores):.4f})')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_feature_importance(model_name, model_dir, feature_cols):\n",
    "    \n",
    "    model_files = glob(f\"{model_dir}/{model_name}_fold_*.pkl\")\n",
    "    models = [joblib.load(model_file) for model_file in model_files]\n",
    "\n",
    "    df_importance = []\n",
    "    \n",
    "    for model in models:\n",
    "        feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':feature_cols})\n",
    "        feature_imp.sort_values(by='Value', ascending=False, inplace=True)\n",
    "        df_importance.append(feature_imp)\n",
    "        \n",
    "    df_importance = pd.concat(df_importance)\n",
    "    df_importance = df_importance.groupby('Feature').mean().reset_index()\n",
    "\n",
    "    df_importance.sort_values(by='Value', ascending=False, inplace=True)\n",
    "    df_importance = df_importance.reset_index(drop=True)\n",
    "    \n",
    "    return df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance = calc_feature_importance('small_lgb_cv', '../data',  feature_cols)\n",
    "imp_thred = np.percentile(df_importance['Value'].values, 20)\n",
    "less_important = df_importance[df_importance['Value'] < imp_thred]\n",
    "\n",
    "print(f\"Importance Threshold (20 percentile): {imp_thred}\")\n",
    "print(f\"Number of less important features: {len(less_important)}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 11))\n",
    "\n",
    "less_imp_v1 = less_important[less_important['Feature'].isin(feature_dicts['v1_features']+feature_dicts['v1_feature_category'])]\n",
    "less_imp_v2 = less_important[less_important['Feature'].isin(feature_dicts['v2_features'])]\n",
    "less_imp_v3 = less_important[less_important['Feature'].isin(feature_dicts['v3_features'])]\n",
    "\n",
    "for i, (ax, df_lss) in enumerate(zip(axes, [less_imp_v1, less_imp_v2, less_imp_v3])):\n",
    "    sns.barplot(x=\"Value\", y=\"Feature\", data=df_lss, ax=ax)\n",
    "    ax.grid()\n",
    "    ax.set_title(f\"V{i+1} Features\")\n",
    "# ax.set_xlim(0, 2000)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
