{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import talib as ta\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "import os, sys, warnings\n",
    "from time import time \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "1. Wrap up one function for feature generation \n",
    "2. Wrap up one function for model training \n",
    "3. Within the submission API, consider proper concat of cache and new test \n",
    "    \n",
    "    * V1 features are row-based --> don't need cache. \n",
    "    * V2 features are based on sec_in_bucket (cross-section feat) --> don't need cache. (Perhaps the gen_v2_features function should be changed to not using groupby() ? )\n",
    "    * V3 features requires timeseries data -->  the cache is needed.\n",
    "    * The cache should save the timeseries records of all stocks with sufficient length. \n",
    "    * The row after concat should be re-ordered. \n",
    "    * After calculate features, only the current seconds_in_bucket should be returned. \n",
    "    * Standarization: in the training phase, the standarization is implemented on a multi-day scale. In the cached dataset, only limited timesteps are used in Standarization. \n",
    "    * Standarization: during training, perhaps we should perform Standarization on cross-section only. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    # Calculate the initial memory usage of the DataFrame\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                # Check if the column's data type is a float\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                    \n",
    "    if verbose:\n",
    "        print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_v1_features(df, prices):\n",
    "\n",
    "    # V1 features: directly apply formula to a single row\n",
    "    v1_features = {\n",
    "        \"volume\": \"ask_size + bid_size\",\n",
    "        \"mid_price\": \"(ask_price + bid_price)/2\",\n",
    "        \"liquidity_imbalance\": \"(bid_size-ask_size)/(bid_size+ask_size)\",\n",
    "        \"matched_imbalance\": \"(imbalance_size - matched_size)/(matched_size+imbalance_size)\",\n",
    "        \"size_imbalance\": \"bid_size / ask_size\",\n",
    "        \"imbalance_intensity\": \"imbalance_size / volume\",\n",
    "        \"matched_intensity\": \"matched_size / volume\",\n",
    "        \"price_spread\": \"ask_price - bid_price\",\n",
    "        'market_urgency': 'price_spread * liquidity_imbalance',\n",
    "        'depth_pressure': '(ask_size - bid_size) * (far_price - near_price)',\n",
    "        'price_pressure': 'imbalance_size * (ask_price - bid_price)',\n",
    "        'imbalance_with_flag': 'imbalance_size * imbalance_buy_sell_flag',\n",
    "    }\n",
    "\n",
    "    # include pair-wise price imbalances\n",
    "    for c in combinations(prices, 2):\n",
    "        v1_features[f\"{c[0]}_{c[1]}_imbalance\"] = f\"({c[0]} - {c[1]}) / ({c[0]} + {c[1]})\"\n",
    "    \n",
    "    for k, v in v1_features.items():\n",
    "        df[k] = df.eval(v)\n",
    "        \n",
    "    v1_feature_category = {\n",
    "        'minute': 'seconds_in_bucket // 60',\n",
    "        'imb_buy_side': \"(imbalance_buy_sell_flag == 1)\",\n",
    "        'imb_sell_side': \"(imbalance_buy_sell_flag == -1)\",\n",
    "        'first_half_session': '(seconds_in_bucket <= 240)',\n",
    "        'second_half_session': '(seconds_in_bucket > 240)'\n",
    "    }\n",
    "    \n",
    "    for k, v in v1_feature_category.items():\n",
    "        df[k] = df.eval(v).astype(np.int8)\n",
    "        \n",
    "    df = reduce_mem_usage(df, verbose=0)\n",
    "        \n",
    "    return df, list(v1_features.keys()), list(v1_feature_category.keys())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_v2_features(df, v2_feat_cols):\n",
    "    \n",
    "    # V2 features: cross-section features\n",
    "    # V2 features are generated on the groupby(['date_id', 'seconds_in_bucket'])\n",
    "    # These features includes:\n",
    "    # 1. statistics of V1 features (non-categorical)\n",
    "    # 2. rank of V1 features for each stocks (non-categorical)\n",
    "    \n",
    "    group = df.groupby(['date_id', 'seconds_in_bucket'])\n",
    "\n",
    "    v2_features_stats = ['mean', 'median', 'std', 'min', 'max']\n",
    "\n",
    "    # calculate statistics of V1 features for each stock\n",
    "    df_v2 = group[v2_feat_cols].agg(v2_features_stats).reset_index()\n",
    "    df_v2.columns = ['date_id', 'seconds_in_bucket'] + [f\"{c[1]}_{c[0]}\" for c in df_v2.columns[2:]]\n",
    "    df = df.merge(df_v2, on=['date_id', 'seconds_in_bucket'], how='left')\n",
    "    \n",
    "\n",
    "    # calculate rank of V1 features for each stock\n",
    "    df_v2 = group[v2_feat_cols].rank(pct=True).add_prefix('rank_')\n",
    "    df = df.merge(df_v2, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    df = reduce_mem_usage(df, verbose=0)\n",
    "    \n",
    "    v2_features =\\\n",
    "        [f\"{s}_{c}\" for c in v2_feat_cols for s in v2_features_stats] + \\\n",
    "        [f\"rank_{c}\" for c in v2_feat_cols]\n",
    "        \n",
    "    return df, v2_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Requrires at least 11 timesteps to calculate all rolling statistics\n",
    "def gen_v3_features(df, prices, sizes, v1_features):\n",
    "    # V3 features: rolling statistics of V1 features (non-categorical)\n",
    "    # V3 features are generated on the groupby(['date_id', 'stock_id'])\n",
    "    # here we introduce ta-lib functions to calculate TA indicators\n",
    "\n",
    "    # V3.1 relative change of V1 features by shift(1)\n",
    "    # for prices, we calculate the change in basis points (*1e4)\n",
    "    # for other features, we calculate the change in percentage (*1e2)\n",
    "    group_by_stock = df.groupby(['date_id', 'stock_id'])\n",
    "    \n",
    "    relative_price = group_by_stock[prices].pct_change(1).add_prefix('pct_')*1e4\n",
    "    relative_others = group_by_stock[sizes+v1_features].pct_change(1).add_prefix('pct_')*1e2\n",
    "\n",
    "    df = pd.concat([df, relative_price, relative_others], axis=1)\n",
    "    v3_features = list(relative_price.columns) + list(relative_others.columns)\n",
    "    \n",
    "    # V3.2 Simple TA indicators\n",
    "    # Those are simple TA indicators that use only one feature\n",
    "    df_v3 = group_by_stock[prices + sizes + v1_features].rolling(5).agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "    stats_cols = [f\"{c[1]}_{c[0]}_5\" for c in df_v3.columns[2:]]\n",
    "    df_v3.columns = ['date_id', 'stock_id'] + stats_cols\n",
    "    df_v3.set_index('_level_2_5', inplace=True)\n",
    "    df_v3.drop(columns=['date_id', 'stock_id'], inplace=True)\n",
    "    \n",
    "    df = df.merge(df_v3, left_index=True, right_index=True, how='left')\n",
    "    v3_features += df_v3.columns.tolist()\n",
    "        \n",
    "    # # V3.3 TA indicators that use multiple features\n",
    "    def composite_ta(df):\n",
    "\n",
    "        ad_osc = ta.ADOSC(df['ask_price'], df['bid_price'], df['wap'], df['volume'], fastperiod=3, slowperiod=5)\n",
    "        macd, macdsignal, macdhist = ta.MACD(df['wap'], fastperiod=5, slowperiod=11, signalperiod=3)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'ema': ta.EMA(df['wap'], timeperiod=5),\n",
    "            'rsi': ta.RSI(df['wap'], timeperiod=5),\n",
    "            'cci': ta.CCI(df['ask_price'], df['bid_price'], df['wap'], timeperiod=5),\n",
    "            'mfi': ta.MFI(df['ask_price'], df['bid_price'], df['wap'], df['volume'], timeperiod=5),\n",
    "            'ad_osc': ad_osc,\n",
    "            'macd': macd,\n",
    "            'macdsignal': macdsignal,\n",
    "            'macdhist': macdhist\n",
    "        })\n",
    "    \n",
    "    df_v3 = group_by_stock.apply(composite_ta) \n",
    "    v3_features += df_v3.columns.tolist()\n",
    "    \n",
    "    df_v3.reset_index(inplace=True)\n",
    "    df_v3.set_index('level_2', inplace=True)\n",
    "    df_v3.drop(columns=['date_id', 'stock_id'], inplace=True)\n",
    "    \n",
    "    df = pd.concat([df, df_v3], axis=1)\n",
    "    \n",
    "    return df, v3_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "df = df[~df['target'].isnull()] \n",
    "\n",
    "print(df.shape)\n",
    "print(f\"Trading days: {df['date_id'].nunique()}\")\n",
    "print(f\"Stocks: {df['stock_id'].nunique()}\")\n",
    "\n",
    "df['far_price'] = df['far_price'].fillna(0)\n",
    "df['near_price'] = df['near_price'].fillna(0)\n",
    "\n",
    "df = reduce_mem_usage(df, verbose=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate running time: 9min\n",
    "\n",
    "prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "categorical_cols = [\"stock_id\", \"seconds_in_bucket\", 'imbalance_buy_sell_flag']\n",
    "\n",
    "feature_cols = prices + sizes\n",
    "\n",
    "df, v1_features, v1_feature_category = gen_v1_features(df, prices)\n",
    "feature_cols += v1_features\n",
    "categorical_cols += v1_feature_category\n",
    "\n",
    "df, v2_features = gen_v2_features(df, prices+sizes+v1_features)\n",
    "feature_cols += v2_features\n",
    "\n",
    "df, v3_features = gen_v3_features(df, prices, sizes, v1_features)\n",
    "feature_cols += v3_features\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "df = reduce_mem_usage(df, verbose=1)\n",
    "\n",
    "print(len(feature_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is any missing values or infinite values\n",
    "check_invalid_values = pd.DataFrame(columns=['null', 'inf'])\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        check_invalid_values.loc[col] = [df[col].isnull().sum(), np.isinf(df[col]).sum()]\n",
    "    except:\n",
    "        print(\"Skip column: \", col)\n",
    "        pass\n",
    "    \n",
    "check_invalid_values.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"/home/lishi/projects/Competition/data/train_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import joblib, gc\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df, feature_cols):\n",
    "    scaler = StandardScaler()\n",
    "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set lgb parameters\n",
    "lgb_params = {\n",
    "    'learning_rate': 0.009,#0.018,\n",
    "    'max_depth': 10,#9,\n",
    "    'n_estimators': 700,#600,\n",
    "    'num_leaves': 500,#440,\n",
    "    'objective': 'mae',\n",
    "    'random_state': 42,\n",
    "    'reg_alpha': 0.01,\n",
    "    'reg_lambda': 0.01,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'num_threads': 24\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV strategy: KFold\n",
    "# split train and valid set by date_id\n",
    "# Within each fold, a continuous period of n days is used as validation set\n",
    "# The start date of validation set is shifted by n day for each fold\n",
    "# n = total_days / n_fold\n",
    "\n",
    "k_fold = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "kf_split = k_fold.split(df['date_id'].unique())\n",
    "\n",
    "mae_scores = []\n",
    "models = []\n",
    "\n",
    "feature_names = categorical_cols + feature_cols\n",
    "print(f\"Length of feature names: {len(feature_names)}\")\n",
    "\n",
    "for fold, (train_dates, valid_dates) in enumerate(kf_split):\n",
    "    \n",
    "    print(f\"Fold {fold+1}\")\n",
    "    fold_start = time()\n",
    "    \n",
    "    # split train and valid set\n",
    "    df_train = df[df[\"date_id\"].isin(train_dates)]\n",
    "    df_valid = df[df[\"date_id\"].isin(valid_dates)]\n",
    "    \n",
    "    print(f\"Train : {df_train.shape}, Valid : {df_valid.shape}\")\n",
    "    \n",
    "    df_train_feature = df_train[feature_names]\n",
    "    df_train_feature = standardize(df_train_feature, feature_cols)\n",
    "    \n",
    "    print(f\"Train feature: {x_train.shape}, Train target: {y_train.shape}\")\n",
    "    \n",
    "    df_valid_feature = df_valid[feature_names]\n",
    "    df_valid_feature = standardize(df_valid_feature, feature_cols)\n",
    "    \n",
    "    print(f\"Valid feature: {x_valid.shape}, Valid target: {y_valid.shape}\")\n",
    "    \n",
    "    # train_data = lgb.Dataset(\n",
    "    #     data=x_train, \n",
    "    #     label=y_train, \n",
    "    #     feature_name=feature_names,\n",
    "    #     categorical_feature=categorical_cols\n",
    "    #     )\n",
    "    \n",
    "    # valid_data = lgb.Dataset(\n",
    "    #     data=x_valid, \n",
    "    #     label=y_valid, \n",
    "    #     feature_name=feature_names,\n",
    "    #     categorical_feature=categorical_cols\n",
    "    #     )\n",
    "    \n",
    "    print(f\"Data preparation finished. Time elapsed: {time()-fold_start:.2f}s\")\n",
    "    \n",
    "    print(\"Start training...\")\n",
    "    training_start = time()\n",
    "    \n",
    "    # model = lgb.train(lgb_params, train_data, valid_sets=[train_data, valid_data])\n",
    "    lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_model.fit(\n",
    "        df_train_feature, \n",
    "        df_train['target'],\n",
    "        eval_set=[(df_valid_feature, df_valid['target'])],\n",
    "        feature_name = feature_names,\n",
    "        categorical_feature = categorical_cols_less,\n",
    "        callbacks=[\n",
    "            lgb.callback.early_stopping(stopping_rounds=100),\n",
    "            lgb.callback.log_evaluation(period=100),\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    models.append(lgb_model)\n",
    "    \n",
    "    model_file = f\"../data/lgb_regressor_fold_{fold+1}.pkl\" \n",
    "    joblib.dump(lgb_model, model_file)\n",
    "    \n",
    "    print(f\"Fold {fold+1} Trainning finished. Time elapsed: {time()-training_start:.2f}s\")\n",
    "    \n",
    "    y_pred_valid = lgb_model.predict(df_valid_feature)\n",
    "    mae = mean_absolute_error(df_valid['target'], y_pred_valid)\n",
    "    mae_scores.append(mae)\n",
    "\n",
    "    print(f\"Fold {fold+1} MAE: {mae}\")\n",
    "    print(f\"Fold {fold+1} Time elapsed: {time()-fold_start:.2f}s\")\n",
    "    \n",
    "    del df_train, df_valid, df_train_feature, df_valid_feature\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"Overall MAE: {np.mean(mae_scores)}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mae scores\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "plt.plot(mae_scores, marker='o', color='blue', label='MAE')\n",
    "plt.text(0.95, 0.95, f\"Overall MAE: {np.mean(mae_scores):.4f}\", fontsize=12, ha='right', va='top', color='red', transform=plt.gca().transAxes)\n",
    "plt.title('MAE Scores')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance using Gain\n",
    "# fig = plt.figure()\n",
    "# lgb.plot_importance(model, importance_type=\"gain\", title=\"LightGBM Feature Importance (Gain)\")\n",
    "# plt.show()\n",
    "feature_imp = pd.DataFrame({'Value':lgb_model.feature_importances_,'Feature':feature_names})\n",
    "feature_imp.sort_values(by='Value', ascending=False, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(8, 16))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.iloc[-60:])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV strategy: KFold\n",
    "# split train and valid set by date_id\n",
    "# Within each fold, a continuous period of n days is used as validation set\n",
    "# The start date of validation set is shifted by n day for each fold\n",
    "# n = total_days / n_fold\n",
    "\n",
    "# remove the least important features (based on feature importance)\n",
    "remove_features = feature_imp.iloc[-60:].Feature.tolist()\n",
    "\n",
    "k_fold = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "kf_split = k_fold.split(df['date_id'].unique())\n",
    "\n",
    "mae_scores = []\n",
    "models = []\n",
    "\n",
    "feature_names = [x for x in categorical_cols + feature_cols if x not in remove_features]\n",
    "feature_cols_less = [x for x in feature_cols if x not in remove_features]\n",
    "categorical_cols_less = [x for x in categorical_cols if x not in remove_features]\n",
    "\n",
    "print(f\"Length of feature names: {len(feature_names)}\")\n",
    "\n",
    "for fold, (train_dates, valid_dates) in enumerate(kf_split):\n",
    "    \n",
    "    print(f\"Fold {fold+1}\")\n",
    "    fold_start = time()\n",
    "    \n",
    "    # split train and valid set\n",
    "    df_train = df[df[\"date_id\"].isin(train_dates)]\n",
    "    df_valid = df[df[\"date_id\"].isin(valid_dates)]\n",
    "    \n",
    "    print(f\"Train : {df_train.shape}, Valid : {df_valid.shape}\")\n",
    "    \n",
    "    df_train_feature = df_train[feature_names]\n",
    "    df_train_feature = standardize(df_train_feature, feature_cols_less)\n",
    "    \n",
    "    print(f\"Train feature: {x_train.shape}, Train target: {y_train.shape}\")\n",
    "    \n",
    "    df_valid_feature = df_valid[feature_names]\n",
    "    df_valid_feature = standardize(df_valid_feature, feature_cols_less)\n",
    "    \n",
    "    print(f\"Valid feature: {x_valid.shape}, Valid target: {y_valid.shape}\")\n",
    "    \n",
    "    print(f\"Data preparation finished. Time elapsed: {time()-fold_start:.2f}s\")\n",
    "    \n",
    "    print(\"Start training...\")\n",
    "    training_start = time()\n",
    "    \n",
    "    # model = lgb.train(lgb_params, train_data, valid_sets=[train_data, valid_data])\n",
    "    lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_model.fit(\n",
    "        df_train_feature, \n",
    "        df_train['target'],\n",
    "        eval_set=[(df_valid_feature, df_valid['target'])],\n",
    "        feature_name = feature_names,\n",
    "        categorical_feature = categorical_cols_less,\n",
    "        callbacks=[\n",
    "            lgb.callback.early_stopping(stopping_rounds=100),\n",
    "            lgb.callback.log_evaluation(period=100),\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    models.append(lgb_model)\n",
    "    \n",
    "    model_file = f\"../data/lgb_regressor_fold_{fold+1}_feat_400.pkl\" \n",
    "    joblib.dump(lgb_model, model_file)\n",
    "    \n",
    "    print(f\"Fold {fold+1} Trainning finished. Time elapsed: {time()-training_start:.2f}s\")\n",
    "    \n",
    "    y_pred_valid = lgb_model.predict(df_valid_feature)\n",
    "    mae = mean_absolute_error(df_valid['target'], y_pred_valid)\n",
    "    mae_scores.append(mae)\n",
    "\n",
    "    print(f\"Fold {fold+1} MAE: {mae}\")\n",
    "    print(f\"Fold {fold+1} Time elapsed: {time()-fold_start:.2f}s\")\n",
    "    \n",
    "    del df_train, df_valid, df_train_feature, df_valid_feature\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"Overall MAE: {np.mean(mae_scores)}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mae scores\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "plt.plot(mae_scores, marker='o', color='blue', label='MAE')\n",
    "plt.text(0.95, 0.95, f\"Overall MAE: {np.mean(mae_scores):.4f}\", fontsize=12, ha='right', va='top', color='red', transform=plt.gca().transAxes)\n",
    "plt.title('MAE Scores')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame({'Value':lgb_model.feature_importances_,'Feature':feature_names})\n",
    "feature_imp.sort_values(by='Value', ascending=False, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(8, 16))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.iloc[2:60])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "# add path of optiver2023 package to pythonpath \n",
    "sys.path.append(os.path.abspath('../data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optiver2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "Memory usage of dataframe is 0.47 MB\n",
      "Memory usage after optimization is: 0.35 MB\n",
      "Decreased by 25.41%\n",
      "df_test shape: (200, 463)\n",
      "Feature preprocess done! Time elapsed: 1.00s\n",
      "452\n",
      "You must call `predict()` successfully before you can continue with `iter_test()`\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m categorical_cols \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mstock_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mseconds_in_bucket\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mimbalance_buy_sell_flag\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m feature_cols \u001b[39m=\u001b[39m prices \u001b[39m+\u001b[39m sizes\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m (test, revealed_targets, sample_prediction) \u001b[39min\u001b[39;00m iter_test:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     now_time \u001b[39m=\u001b[39m time()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     test_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(test)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out\n",
    "\n",
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()\n",
    "counter = 0 \n",
    "\n",
    "y_min, y_max = -64, 64\n",
    "qps, predictions = [], []\n",
    "cache = pd.DataFrame()\n",
    "\n",
    "prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "categorical_cols = [\"stock_id\", \"seconds_in_bucket\", 'imbalance_buy_sell_flag']\n",
    "\n",
    "feature_cols = prices + sizes\n",
    "\n",
    "for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "    now_time = time()\n",
    "    \n",
    "    test_len = len(test)\n",
    "    \n",
    "    test, v1_features, v1_feature_category = gen_v1_features(test, prices)\n",
    "    feature_cols += v1_features\n",
    "    categorical_cols += v1_feature_category\n",
    "\n",
    "    test, v2_features = gen_v2_features(test, prices+sizes+v1_features)\n",
    "    feature_cols += v2_features\n",
    "    \n",
    "    cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "    \n",
    "    if counter > 0:\n",
    "        cache = cache.sort_values(['date_id', 'stock_id', 'seconds_in_bucket']).reset_index(drop=True)\n",
    "        dates_unique = cache['date_id'].unique()\n",
    "        cache = cache[cache['date_id'] == dates_unique[-1]].reset_index(drop=True)\n",
    "        print(counter, cache.shape)\n",
    "\n",
    "    cache, v3_features = gen_v3_features(cache, prices, sizes, v1_features)\n",
    "    feature_cols += v3_features\n",
    "\n",
    "    cache.fillna(0, inplace=True)\n",
    "    cache.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    cache = reduce_mem_usage(cache, verbose=1)\n",
    "    \n",
    "    df_test = cache[-test_len:]\n",
    "    \n",
    "    print(f\"df_test shape: {df_test.shape}\")\n",
    "    print(f\"Feature preprocess done! Time elapsed: {time()-now_time:.2f}s\")\n",
    "    \n",
    "    # lgb_predictions = np.zeros(len(cache))\n",
    "    # for model in models:\n",
    "    #     lgb_predictions +=  model.predict(df_test[feature_cols])\n",
    "\n",
    "    print(len(feature_cols))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple validation split: keep the last 45 days for validation\n",
    "\n",
    "# split_day = 435\n",
    "# df_train = df[df[\"date_id\"] <= split_day]\n",
    "# df_valid = df[df[\"date_id\"] > split_day]\n",
    "# print(f\"Train : {df_train.shape}, Valid : {df_valid.shape}\")\n",
    "\n",
    "# df_train_feature = df_train[categorical_cols + feature_cols]\n",
    "# df_train_target = df_train[\"target\"]\n",
    "\n",
    "# df_valid_feature = df_valid[categorical_cols + feature_cols]\n",
    "# df_valid_target = df_valid[\"target\"]\n",
    "\n",
    "# print(f\"Train feature: {df_train_feature.shape}, Train target: {df_train_target.shape}\")\n",
    "# print(f\"Valid feature: {df_valid_feature.shape}, Valid target: {df_valid_target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_feature = standardize(df_train_feature, feature_cols)\n",
    "df_valid_feature = standardize(df_valid_feature, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, features, target):\n",
    "        self.y = target\n",
    "        self.X = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx]\n",
    "    \n",
    "    \n",
    "# define a neural network model of 2 layers\n",
    "# first layer is a non-linear layer with m neurons\n",
    "# second layer is a linear layer with n neuron\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        output = self.linear_relu_stack(x)\n",
    "        return output\n",
    "    \n",
    "# define a function to train the model\n",
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        local_data, local_target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(local_data.float())\n",
    "        loss = criterion(output, local_target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "# define a function for validation\n",
    "def validation(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            local_data, local_target = data.to(device), target.to(device)\n",
    "            output = model(local_data.float())\n",
    "            validation_loss += criterion(output, local_target.float()).item()\n",
    "            y_true.extend(target.tolist())\n",
    "            y_pred.extend(output.tolist())\n",
    "            \n",
    "    validation_loss /= len(val_loader.dataset)\n",
    "    print(f\"\\nValidation set: Average loss: {validation_loss:.6f}\")\n",
    "    print(f\"\\nValidation set: Mean Average Error: {mean_absolute_error(y_true, y_pred):.6f}\\n\")\n",
    "\n",
    "# define a function to predict the target\n",
    "def predict(model, pred_loader):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in pred_loader:\n",
    "            output = model(data.float())\n",
    "            y_pred.extend(output.tolist())\n",
    "            \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(torch.from_numpy(df_train_feature.values), torch.from_numpy(df_train_target.values))\n",
    "valid_dataset = MyDataset(torch.from_numpy(df_valid_feature.values), torch.from_numpy(df_valid_target.values))\n",
    "\n",
    "# print number of data points and number of features\n",
    "print(f\"Number of data points: {train_dataset.X.shape[0]}\")\n",
    "print(f\"Number of features: {train_dataset.X.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(460, 128, 1).to(device)\n",
    "\n",
    "# train the model with train_dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=2048, shuffle=True)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1, 3):\n",
    "    train(model, train_loader, optimizer, criterion, epoch)\n",
    "    validation(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
