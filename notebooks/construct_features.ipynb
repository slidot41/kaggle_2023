{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import talib as ta\n",
    "from itertools import combinations\n",
    "\n",
    "import os, sys, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    # Calculate the initial memory usage of the DataFrame\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                # Check if the column's data type is a float\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                    \n",
    "    if verbose:\n",
    "        print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_v1_features(df, prices):\n",
    "\n",
    "    # V1 features: directly apply formula to a single row\n",
    "    v1_features = {\n",
    "        \"volume\": \"ask_size + bid_size\",\n",
    "        \"mid_price\": \"(ask_price + bid_price)/2\",\n",
    "        \"liquidity_imbalance\": \"(bid_size-ask_size)/(bid_size+ask_size)\",\n",
    "        \"matched_imbalance\": \"(imbalance_size - matched_size)/(matched_size+imbalance_size)\",\n",
    "        \"size_imbalance\": \"bid_size / ask_size\",\n",
    "        \"imbalance_intensity\": \"imbalance_size / volume\",\n",
    "        \"matched_intensity\": \"matched_size / volume\",\n",
    "        \"price_spread\": \"ask_price - bid_price\",\n",
    "        'market_urgency': 'price_spread * liquidity_imbalance',\n",
    "        'depth_pressure': '(ask_size - bid_size) * (far_price - near_price)',\n",
    "        'price_pressure': 'imbalance_size * (ask_price - bid_price)',\n",
    "        'imbalance_with_flag': 'imbalance_size * imbalance_buy_sell_flag',\n",
    "    }\n",
    "\n",
    "    # include pair-wise price imbalances\n",
    "    for c in combinations(prices, 2):\n",
    "        v1_features[f\"{c[0]}_{c[1]}_imbalance\"] = f\"({c[0]} - {c[1]}) / ({c[0]} + {c[1]})\"\n",
    "    \n",
    "    for k, v in v1_features.items():\n",
    "        df[k] = df.eval(v)\n",
    "        \n",
    "    v1_feature_category = {\n",
    "        'minute': 'seconds_in_bucket // 60',\n",
    "        'imb_buy_side': \"(imbalance_buy_sell_flag == 1)\",\n",
    "        'imb_sell_side': \"(imbalance_buy_sell_flag == -1)\",\n",
    "        'first_half_session': '(seconds_in_bucket <= 240)',\n",
    "        'second_half_session': '(seconds_in_bucket > 240)'\n",
    "    }\n",
    "    \n",
    "    for k, v in v1_feature_category.items():\n",
    "        df[k] = df.eval(v).astype(np.int8)\n",
    "        \n",
    "    df = reduce_mem_usage(df, verbose=0)\n",
    "        \n",
    "    return df, list(v1_features.keys()), list(v1_feature_category.keys())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_v2_features(df, v2_feat_cols):\n",
    "    \n",
    "    # V2 features: cross-section features\n",
    "    # V2 features are generated on the groupby(['date_id', 'seconds_in_bucket'])\n",
    "    # These features includes:\n",
    "    # 1. statistics of V1 features (non-categorical)\n",
    "    # 2. rank of V1 features for each stocks (non-categorical)\n",
    "    \n",
    "    group = df.groupby(['date_id', 'seconds_in_bucket'])\n",
    "\n",
    "    v2_features_stats = ['mean', 'median', 'std', 'min', 'max']\n",
    "\n",
    "    # calculate statistics of V1 features for each stock\n",
    "    df_v2 = group[v2_feat_cols].agg(v2_features_stats).reset_index()\n",
    "    df_v2.columns = ['date_id', 'seconds_in_bucket'] + [f\"{c[1]}_{c[0]}\" for c in df_v2.columns[2:]]\n",
    "    df = df.merge(df_v2, on=['date_id', 'seconds_in_bucket'], how='left')\n",
    "    \n",
    "\n",
    "    # calculate rank of V1 features for each stock\n",
    "    df_v2 = group[v2_feat_cols].rank(pct=True).add_prefix('rank_')\n",
    "    df = df.merge(df_v2, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    df = reduce_mem_usage(df, verbose=0)\n",
    "    \n",
    "    v2_features =\\\n",
    "        [f\"{s}_{c}\" for c in v2_feat_cols for s in v2_features_stats] + \\\n",
    "        [f\"rank_{c}\" for c in v2_feat_cols]\n",
    "        \n",
    "    return df, v2_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_v3_features(df, prices, sizes, v1_features):\n",
    "    # V3 features: rolling statistics of V1 features (non-categorical)\n",
    "    # V3 features are generated on the groupby(['date_id', 'stock_id'])\n",
    "    # here we introduce ta-lib functions to calculate TA indicators\n",
    "\n",
    "    # V3.1 relative change of V1 features by shift(1)\n",
    "    # for prices, we calculate the change in basis points (*1e4)\n",
    "    # for other features, we calculate the change in percentage (*1e2)\n",
    "    group_by_stock = df.groupby(['date_id', 'stock_id'])\n",
    "    \n",
    "    relative_price = group_by_stock[prices].pct_change(1).add_prefix('pct_')*1e4\n",
    "    relative_others = group_by_stock[sizes+v1_features].pct_change(1).add_prefix('pct_')*1e2\n",
    "\n",
    "    df = pd.concat([df, relative_price, relative_others], axis=1)\n",
    "    v3_features = list(relative_price.columns) + list(relative_others.columns)\n",
    "    \n",
    "    # V3.2 Simple TA indicators\n",
    "    # Those are simple TA indicators that use only one feature\n",
    "    df_v3 = group_by_stock[prices + sizes + v1_features].rolling(5).agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "    stats_cols = [f\"{c[1]}_{c[0]}_5\" for c in df_v3.columns[2:]]\n",
    "    df_v3.columns = ['date_id', 'stock_id'] + stats_cols\n",
    "    df_v3.set_index('_level_2_5', inplace=True)\n",
    "    df_v3.drop(columns=['date_id', 'stock_id'], inplace=True)\n",
    "    \n",
    "    df = df.merge(df_v3, left_index=True, right_index=True, how='left')\n",
    "    v3_features += df_v3.columns.tolist()\n",
    "        \n",
    "    # # V3.3 TA indicators that use multiple features\n",
    "    def composite_ta(df):\n",
    "\n",
    "        ad_osc = ta.ADOSC(df['ask_price'], df['bid_price'], df['wap'], df['volume'], fastperiod=3, slowperiod=5)\n",
    "        macd, macdsignal, macdhist = ta.MACD(df['wap'], fastperiod=5, slowperiod=11, signalperiod=3)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'ema': ta.EMA(df['wap'], timeperiod=5),\n",
    "            'rsi': ta.RSI(df['wap'], timeperiod=5),\n",
    "            'cci': ta.CCI(df['ask_price'], df['bid_price'], df['wap'], timeperiod=5),\n",
    "            'mfi': ta.MFI(df['ask_price'], df['bid_price'], df['wap'], df['volume'], timeperiod=5),\n",
    "            'ad_osc': ad_osc,\n",
    "            'macd': macd,\n",
    "            'macdsignal': macdsignal,\n",
    "            'macdhist': macdhist\n",
    "        })\n",
    "    \n",
    "    df_v3 = group_by_stock.apply(composite_ta) \n",
    "    v3_features += df_v3.columns.tolist()\n",
    "    \n",
    "    df_v3.reset_index(inplace=True)\n",
    "    df_v3.set_index('level_2', inplace=True)\n",
    "    df_v3.drop(columns=['date_id', 'stock_id'], inplace=True)\n",
    "    \n",
    "    df = pd.concat([df, df_v3], axis=1)\n",
    "    \n",
    "    return df, v3_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5237892, 17)\n",
      "Trading days: 481\n",
      "Stocks: 200\n",
      "Memory usage of dataframe is 719.32 MB\n",
      "Memory usage after optimization is: 344.67 MB\n",
      "Decreased by 52.08%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>imbalance_size</th>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <th>reference_price</th>\n",
       "      <th>matched_size</th>\n",
       "      <th>far_price</th>\n",
       "      <th>near_price</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>bid_size</th>\n",
       "      <th>ask_price</th>\n",
       "      <th>ask_size</th>\n",
       "      <th>wap</th>\n",
       "      <th>target</th>\n",
       "      <th>time_id</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.180603e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>13380277.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>60651.500000</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>8493.030273</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.029704</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666039e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1642214.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>3233.040039</td>\n",
       "      <td>1.000660</td>\n",
       "      <td>20605.089844</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.519986</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.028799e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>1819368.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>37956.000000</td>\n",
       "      <td>1.000298</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-8.389950</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.191768e+07</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000171</td>\n",
       "      <td>18389746.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>2324.899902</td>\n",
       "      <td>1.000214</td>\n",
       "      <td>479032.406250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.010201</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.475500e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>17860614.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>16485.539062</td>\n",
       "      <td>1.000016</td>\n",
       "      <td>434.100006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.349849</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
       "0         0        0                  0    3.180603e+06   \n",
       "1         1        0                  0    1.666039e+05   \n",
       "2         2        0                  0    3.028799e+05   \n",
       "3         3        0                  0    1.191768e+07   \n",
       "4         4        0                  0    4.475500e+05   \n",
       "\n",
       "   imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n",
       "0                        1         0.999812   13380277.00        0.0   \n",
       "1                       -1         0.999896    1642214.25        0.0   \n",
       "2                       -1         0.999561    1819368.00        0.0   \n",
       "3                       -1         1.000171   18389746.00        0.0   \n",
       "4                       -1         0.999532   17860614.00        0.0   \n",
       "\n",
       "   near_price  bid_price      bid_size  ask_price       ask_size  wap  \\\n",
       "0         0.0   0.999812  60651.500000   1.000026    8493.030273  1.0   \n",
       "1         0.0   0.999896   3233.040039   1.000660   20605.089844  1.0   \n",
       "2         0.0   0.999403  37956.000000   1.000298   18995.000000  1.0   \n",
       "3         0.0   0.999999   2324.899902   1.000214  479032.406250  1.0   \n",
       "4         0.0   0.999394  16485.539062   1.000016     434.100006  1.0   \n",
       "\n",
       "     target  time_id row_id  \n",
       "0 -3.029704        0  0_0_0  \n",
       "1 -5.519986        0  0_0_1  \n",
       "2 -8.389950        0  0_0_2  \n",
       "3 -4.010201        0  0_0_3  \n",
       "4 -7.349849        0  0_0_4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/lishi/projects/Competition/data/train.csv\")\n",
    "df = df[~df['target'].isnull()] \n",
    "\n",
    "print(df.shape)\n",
    "print(f\"Trading days: {df['date_id'].nunique()}\")\n",
    "print(f\"Stocks: {df['stock_id'].nunique()}\")\n",
    "\n",
    "df['far_price'] = df['far_price'].fillna(0)\n",
    "df['near_price'] = df['near_price'].fillna(0)\n",
    "\n",
    "df = reduce_mem_usage(df, verbose=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 12278.31 MB\n",
      "Memory usage after optimization is: 9161.28 MB\n",
      "Decreased by 25.39%\n",
      "452\n"
     ]
    }
   ],
   "source": [
    "prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "categorical_cols = [\"stock_id\", \"seconds_in_bucket\", 'imbalance_buy_sell_flag']\n",
    "\n",
    "feature_cols = prices + sizes\n",
    "\n",
    "df, v1_features, v1_feature_category = gen_v1_features(df, prices)\n",
    "feature_cols += v1_features\n",
    "categorical_cols += v1_feature_category\n",
    "\n",
    "df, v2_features = gen_v2_features(df, prices+sizes+v1_features)\n",
    "feature_cols += v2_features\n",
    "\n",
    "df, v3_features = gen_v3_features(df, prices, sizes, v1_features)\n",
    "feature_cols += v3_features\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "df = reduce_mem_usage(df, verbose=1)\n",
    "\n",
    "print(len(feature_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"/home/lishi/projects/Competition/data/train_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_feature[\n",
    "#     (df_train_feature['stock_id']==20) & \n",
    "#     ((df_train_feature['seconds_in_bucket'] ==300) | (df_train_feature['seconds_in_bucket'] ==310) | (df_train_feature['seconds_in_bucket'] ==290))\n",
    "#     # np.isinf(df_train_feature['pct_reference_price_far_price_imbalance'])\n",
    "#     ][\n",
    "#         ['stock_id', 'seconds_in_bucket', 'pct_reference_price_far_price_imbalance', 'reference_price_far_price_imbalance', 'reference_price', 'far_price']\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is any missing values or infinite values\n",
    "# for col in df_train_feature.columns:\n",
    "#     print(col, np.isinf(df_train_feature[col]).sum(), df[col].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df, feature_cols):\n",
    "    scaler = StandardScaler()\n",
    "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  categorical_cols_idx = [df.columns.get_loc(c) for c in categorical_cols]\n",
    "#  categorical_cols_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train : (4719383, 464), Valid : (518509, 464)\n",
      "Train feature: (4719383, 460), Train target: (4719383,)\n",
      "Valid feature: (518509, 460), Valid target: (518509,)\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.425412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 112226\n",
      "[LightGBM] [Info] Number of data points in the train set: 4719383, number of used features: 460\n",
      "[LightGBM] [Info] Start training from score -0.060201\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[699]\ttraining's l1: 6.18817\tvalid_1's l1: 5.18126\n",
      "Fold 1 Trainning finished.\n",
      "Fold 1 MAE: 5.181262397187751\n",
      "Fold 2\n",
      "Train : (4721662, 464), Valid : (516230, 464)\n",
      "Train feature: (4721662, 460), Train target: (4721662,)\n",
      "Valid feature: (516230, 460), Valid target: (516230,)\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.433861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 112199\n",
      "[LightGBM] [Info] Number of data points in the train set: 4721662, number of used features: 460\n",
      "[LightGBM] [Info] Start training from score -0.060201\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb Cell 14\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m train_data \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     data\u001b[39m=\u001b[39mdf_train_feature\u001b[39m.\u001b[39mvalues, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     label\u001b[39m=\u001b[39mdf_train_target\u001b[39m.\u001b[39mvalues, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     feature_name\u001b[39m=\u001b[39mdf_train_feature\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mtolist(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     categorical_feature\u001b[39m=\u001b[39mcategorical_cols\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m valid_data \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     data\u001b[39m=\u001b[39mdf_valid_feature\u001b[39m.\u001b[39mvalues, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     label\u001b[39m=\u001b[39mdf_valid_target\u001b[39m.\u001b[39mvalues, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     feature_name\u001b[39m=\u001b[39mdf_valid_feature\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m     categorical_feature\u001b[39m=\u001b[39mcategorical_cols\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(lgb_params, train_data, valid_sets\u001b[39m=\u001b[39;49m[train_data, valid_data])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Trainning finished.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bifu-eo-srv-dell.ethz.ch/home/lishi/projects/Competition/kaggle_2023/notebooks/construct_features.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m model_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/home/lishi/projects/Competition/data/lgb_model_fold_\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m \n",
      "File \u001b[0;32m~/miniconda3/envs/dsml/lib/python3.10/site-packages/lightgbm/engine.py:276\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    269\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[1;32m    270\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    271\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m    272\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[1;32m    273\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[1;32m    274\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 276\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[1;32m    278\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[39m=\u001b[39m []\n\u001b[1;32m    279\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dsml/lib/python3.10/site-packages/lightgbm/basic.py:3658\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3656\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3657\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 3658\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle,\n\u001b[1;32m   3660\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[1;32m   3661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3662\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set lgb parameters\n",
    "\n",
    "lgb_params = {\n",
    "    'learning_rate': 0.009,#0.018,\n",
    "    'max_depth': 10,#9,\n",
    "    'n_estimators': 700,#600,\n",
    "    'num_leaves': 500,#440,\n",
    "    'objective': 'mae',\n",
    "    'random_state': 42,\n",
    "    'reg_alpha': 0.01,\n",
    "    'reg_lambda': 0.01,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'num_threads': 24\n",
    "    }\n",
    "\n",
    "# CV strategy: KFold\n",
    "# split train and valid set by date_id\n",
    "# Within each fold, a continuous period of n days is used as validation set\n",
    "# The start date of validation set is shifted by n day for each fold\n",
    "# n = total_days / n_fold\n",
    "\n",
    "k_fold = KFold(n_splits=10, shuffle=False, random_state=None)\n",
    "kf_split = k_fold.split(df['date_id'].unique())\n",
    "\n",
    "mae_scores = []\n",
    "\n",
    "for fold, (train_dates, valid_dates) in enumerate(kf_split):\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    \n",
    "    # split train and valid set\n",
    "    df_train = df[df[\"date_id\"].isin(train_dates)]\n",
    "    df_valid = df[df[\"date_id\"].isin(valid_dates)]\n",
    "    \n",
    "    print(f\"Train : {df_train.shape}, Valid : {df_valid.shape}\")\n",
    "    \n",
    "    df_train_feature = df_train[categorical_cols + feature_cols]\n",
    "    df_train_target = df_train[\"target\"]\n",
    "    \n",
    "    df_valid_feature = df_valid[categorical_cols + feature_cols]\n",
    "    df_valid_target = df_valid[\"target\"]\n",
    "    \n",
    "    print(f\"Train feature: {df_train_feature.shape}, Train target: {df_train_target.shape}\")\n",
    "    print(f\"Valid feature: {df_valid_feature.shape}, Valid target: {df_valid_target.shape}\")\n",
    "    \n",
    "    # standardize the features\n",
    "    df_train_feature = standardize(df_train_feature, feature_cols)\n",
    "    df_valid_feature = standardize(df_valid_feature, feature_cols)\n",
    "    \n",
    "    # categorical_cols_idx = [df_train_feature.columns.get_loc(c) for c in categorical_cols]\n",
    "    # params['categorical_feature'] = categorical_cols_idx\n",
    "    \n",
    "    train_data = lgb.Dataset(\n",
    "        data=df_train_feature.values, \n",
    "        label=df_train_target.values, \n",
    "        feature_name=df_train_feature.columns.tolist(), \n",
    "        categorical_feature=categorical_cols\n",
    "        )\n",
    "    \n",
    "    valid_data = lgb.Dataset(\n",
    "        data=df_valid_feature.values, \n",
    "        label=df_valid_target.values, \n",
    "        feature_name=df_valid_feature.columns.tolist(),\n",
    "        categorical_feature=categorical_cols\n",
    "        )\n",
    "    \n",
    "    model = lgb.train(lgb_params, train_data, valid_sets=[train_data, valid_data])\n",
    "    \n",
    "    print(f\"Fold {fold+1} Trainning finished.\")\n",
    "    \n",
    "    model_file = f\"/home/lishi/projects/Competition/data/lgb_model_fold_{fold+1}.pkl\" \n",
    "    joblib.dump(model, model_file)\n",
    "    \n",
    "    y_pred_valid = model.predict(df_valid_feature.values)\n",
    "\n",
    "    y_pred_valid = np.nan_to_num(y_pred_valid)\n",
    "    # y_valid = np.nan_to_num(df_valid_target.values)\n",
    "    \n",
    "    mae = mean_absolute_error(df_valid_target.values, y_pred_valid)\n",
    "    mae_scores.append(mae)\n",
    "\n",
    "    print(f\"Fold {fold+1} MAE: {mae}\")\n",
    "\n",
    "print(f\"Overall MAE: {np.mean(mae_scores)}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple validation split: keep the last 45 days for validation\n",
    "\n",
    "# split_day = 435\n",
    "# df_train = df[df[\"date_id\"] <= split_day]\n",
    "# df_valid = df[df[\"date_id\"] > split_day]\n",
    "# print(f\"Train : {df_train.shape}, Valid : {df_valid.shape}\")\n",
    "\n",
    "# df_train_feature = df_train[categorical_cols + feature_cols]\n",
    "# df_train_target = df_train[\"target\"]\n",
    "\n",
    "# df_valid_feature = df_valid[categorical_cols + feature_cols]\n",
    "# df_valid_target = df_valid[\"target\"]\n",
    "\n",
    "# print(f\"Train feature: {df_train_feature.shape}, Train target: {df_train_target.shape}\")\n",
    "# print(f\"Valid feature: {df_valid_feature.shape}, Valid target: {df_valid_target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_feature = standardize(df_train_feature, feature_cols)\n",
    "df_valid_feature = standardize(df_valid_feature, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, features, target):\n",
    "        self.y = target\n",
    "        self.X = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx]\n",
    "    \n",
    "    \n",
    "# define a neural network model of 2 layers\n",
    "# first layer is a non-linear layer with m neurons\n",
    "# second layer is a linear layer with n neuron\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        output = self.linear_relu_stack(x)\n",
    "        return output\n",
    "    \n",
    "# define a function to train the model\n",
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        local_data, local_target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(local_data.float())\n",
    "        loss = criterion(output, local_target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "# define a function for validation\n",
    "def validation(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            local_data, local_target = data.to(device), target.to(device)\n",
    "            output = model(local_data.float())\n",
    "            validation_loss += criterion(output, local_target.float()).item()\n",
    "            y_true.extend(target.tolist())\n",
    "            y_pred.extend(output.tolist())\n",
    "            \n",
    "    validation_loss /= len(val_loader.dataset)\n",
    "    print(f\"\\nValidation set: Average loss: {validation_loss:.6f}\")\n",
    "    print(f\"\\nValidation set: Mean Average Error: {mean_absolute_error(y_true, y_pred):.6f}\\n\")\n",
    "\n",
    "# define a function to predict the target\n",
    "def predict(model, pred_loader):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in pred_loader:\n",
    "            output = model(data.float())\n",
    "            y_pred.extend(output.tolist())\n",
    "            \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 4742893\n",
      "Number of features: 460\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(torch.from_numpy(df_train_feature.values), torch.from_numpy(df_train_target.values))\n",
    "valid_dataset = MyDataset(torch.from_numpy(df_valid_feature.values), torch.from_numpy(df_valid_target.values))\n",
    "\n",
    "# print number of data points and number of features\n",
    "print(f\"Number of data points: {train_dataset.X.shape[0]}\")\n",
    "print(f\"Number of features: {train_dataset.X.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(460, 128, 1).to(device)\n",
    "\n",
    "# train the model with train_dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=2048, shuffle=True)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1, 3):\n",
    "    train(model, train_loader, optimizer, criterion, epoch)\n",
    "    validation(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4742893 (0%)]\tLoss: 84.258804\n",
      "Train Epoch: 1 [204800/4742893 (4%)]\tLoss: 84.634796\n",
      "Train Epoch: 1 [409600/4742893 (9%)]\tLoss: 87.189972\n",
      "Train Epoch: 1 [614400/4742893 (13%)]\tLoss: 85.591690\n",
      "Train Epoch: 1 [819200/4742893 (17%)]\tLoss: 95.302406\n",
      "Train Epoch: 1 [1024000/4742893 (22%)]\tLoss: 88.084938\n",
      "Train Epoch: 1 [1228800/4742893 (26%)]\tLoss: 92.259865\n",
      "Train Epoch: 1 [1433600/4742893 (30%)]\tLoss: 92.992401\n",
      "Train Epoch: 1 [1638400/4742893 (35%)]\tLoss: 108.388931\n",
      "Train Epoch: 1 [1843200/4742893 (39%)]\tLoss: 93.323479\n",
      "Train Epoch: 1 [2048000/4742893 (43%)]\tLoss: 88.264053\n",
      "Train Epoch: 1 [2252800/4742893 (47%)]\tLoss: 90.768387\n",
      "Train Epoch: 1 [2457600/4742893 (52%)]\tLoss: 103.272415\n",
      "Train Epoch: 1 [2662400/4742893 (56%)]\tLoss: 84.189812\n",
      "Train Epoch: 1 [2867200/4742893 (60%)]\tLoss: 85.940292\n",
      "Train Epoch: 1 [3072000/4742893 (65%)]\tLoss: 86.421501\n",
      "Train Epoch: 1 [3276800/4742893 (69%)]\tLoss: 86.656960\n",
      "Train Epoch: 1 [3481600/4742893 (73%)]\tLoss: 87.373184\n",
      "Train Epoch: 1 [3686400/4742893 (78%)]\tLoss: 84.794907\n",
      "Train Epoch: 1 [3891200/4742893 (82%)]\tLoss: 83.990005\n",
      "Train Epoch: 1 [4096000/4742893 (86%)]\tLoss: 85.714775\n",
      "Train Epoch: 1 [4300800/4742893 (91%)]\tLoss: 79.695602\n",
      "Train Epoch: 1 [4505600/4742893 (95%)]\tLoss: 84.020157\n",
      "Train Epoch: 1 [4710400/4742893 (99%)]\tLoss: 85.971977\n",
      "\n",
      "Validation set: Average loss: 0.039043\n",
      "\n",
      "Validation set: Mean Average Error: 5.959985\n",
      "\n",
      "Train Epoch: 2 [0/4742893 (0%)]\tLoss: 89.608627\n",
      "Train Epoch: 2 [204800/4742893 (4%)]\tLoss: 82.094452\n",
      "Train Epoch: 2 [409600/4742893 (9%)]\tLoss: 91.825706\n",
      "Train Epoch: 2 [614400/4742893 (13%)]\tLoss: 93.626068\n",
      "Train Epoch: 2 [819200/4742893 (17%)]\tLoss: 86.356323\n",
      "Train Epoch: 2 [1024000/4742893 (22%)]\tLoss: 82.657135\n",
      "Train Epoch: 2 [1228800/4742893 (26%)]\tLoss: 90.111343\n",
      "Train Epoch: 2 [1433600/4742893 (30%)]\tLoss: 99.527954\n",
      "Train Epoch: 2 [1638400/4742893 (35%)]\tLoss: 84.103149\n",
      "Train Epoch: 2 [1843200/4742893 (39%)]\tLoss: 83.437706\n",
      "Train Epoch: 2 [2048000/4742893 (43%)]\tLoss: 82.737701\n",
      "Train Epoch: 2 [2252800/4742893 (47%)]\tLoss: 86.191147\n",
      "Train Epoch: 2 [2457600/4742893 (52%)]\tLoss: 89.634903\n",
      "Train Epoch: 2 [2662400/4742893 (56%)]\tLoss: 96.261322\n",
      "Train Epoch: 2 [2867200/4742893 (60%)]\tLoss: 93.999794\n",
      "Train Epoch: 2 [3072000/4742893 (65%)]\tLoss: 89.184868\n",
      "Train Epoch: 2 [3276800/4742893 (69%)]\tLoss: 97.317169\n",
      "Train Epoch: 2 [3481600/4742893 (73%)]\tLoss: 97.047485\n",
      "Train Epoch: 2 [3686400/4742893 (78%)]\tLoss: 91.572830\n",
      "Train Epoch: 2 [3891200/4742893 (82%)]\tLoss: 85.868752\n",
      "Train Epoch: 2 [4096000/4742893 (86%)]\tLoss: 82.728470\n",
      "Train Epoch: 2 [4300800/4742893 (91%)]\tLoss: 97.153114\n",
      "Train Epoch: 2 [4505600/4742893 (95%)]\tLoss: 88.894821\n",
      "Train Epoch: 2 [4710400/4742893 (99%)]\tLoss: 80.064835\n",
      "\n",
      "Validation set: Average loss: 0.039039\n",
      "\n",
      "Validation set: Mean Average Error: 5.958813\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
